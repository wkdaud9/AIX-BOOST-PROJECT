# -*- coding: utf-8 -*-
"""
ê³µì§€ì‚¬í•­ ë¶„ì„ê¸° ëª¨ë“ˆ

ì´ íŒŒì¼ì´ í•˜ëŠ” ì¼:
í¬ë¡¤ë§í•œ ê³µì§€ì‚¬í•­ì„ Gemini AIë¡œ ë¶„ì„í•´ì„œ ìœ ìš©í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
ì˜ˆë¥¼ ë“¤ì–´, ê¸´ ê³µì§€ì‚¬í•­ì„ ìš”ì•½í•˜ê±°ë‚˜, ì–´ë–¤ ì¹´í…Œê³ ë¦¬ì¸ì§€ íŒë‹¨í•˜ê±°ë‚˜,
ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€ ì ìˆ˜ë¥¼ ë§¤ê¹ë‹ˆë‹¤.

ë¹„ìœ :
- ê³µì§€ì‚¬í•­ = í•™êµì—ì„œ ë°›ì€ ê¸´ ê°€ì •í†µì‹ ë¬¸
- ì´ ë¶„ì„ê¸° = ê°€ì •í†µì‹ ë¬¸ì„ ì½ê³  ì¤‘ìš”í•œ ë¶€ë¶„ë§Œ í˜•ê´‘íœìœ¼ë¡œ í‘œì‹œí•´ì£¼ëŠ” ì¹œêµ¬
"""

from typing import Dict, Any, List, Optional
from .gemini_client import GeminiClient
from . import prompts
import json
import time
import re
import requests
import google.generativeai as genai
from PIL import Image
from io import BytesIO
from datetime import datetime


class NoticeAnalyzer:
    """
    ê³µì§€ì‚¬í•­ì„ AIë¡œ ë¶„ì„í•˜ëŠ” í´ë˜ìŠ¤

    ğŸ¯ ëª©ì :
    Gemini AIë¥¼ í™œìš©í•˜ì—¬ ê³µì§€ì‚¬í•­ì˜ ë‹¤ì–‘í•œ ì •ë³´ë¥¼ ìë™ìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.

    ğŸ—ï¸ ì£¼ìš” ê¸°ëŠ¥:
    1. analyze_notice: ê³µì§€ì‚¬í•­ ì¢…í•© ë¶„ì„ (ìš”ì•½, ì¹´í…Œê³ ë¦¬, ì¤‘ìš”ë„ í•œë²ˆì—)
    2. extract_summary: ìš”ì•½ë§Œ ì¶”ì¶œ
    3. categorize: ì¹´í…Œê³ ë¦¬ë§Œ íŒë‹¨
    4. calculate_importance: ì¤‘ìš”ë„ë§Œ ê³„ì‚°
    5. extract_keywords: í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
    """

    # ì§€ì›í•˜ëŠ” ì¹´í…Œê³ ë¦¬ ëª©ë¡
    CATEGORIES = [
        "í•™ì‚¬",      # ìˆ˜ê°•ì‹ ì²­, í•™ì , ì„±ì , ì¡¸ì—… ë“±
        "ì¥í•™",      # ì¥í•™ê¸ˆ, í•™ìê¸ˆ ëŒ€ì¶œ, ë“±ë¡ê¸ˆ ë“±
        "ì·¨ì—…",      # ì±„ìš©, ì¸í„´ì‹­, ì·¨ì—…ë°•ëŒíšŒ ë“±
        "í–‰ì‚¬",      # ì…í•™ì‹, ì¡¸ì—…ì‹, ì¶•ì œ, ì˜¤ë¦¬ì—”í…Œì´ì…˜ ë“±
        "êµìœ¡",      # íŠ¹ê°•, êµìœ¡ í”„ë¡œê·¸ë¨, ì§„ë¡œ êµìœ¡, ì„¸ë¯¸ë‚˜ ë“±
        "ê³µëª¨ì „"     # ëŒ€íšŒ, ê²½ì§„ëŒ€íšŒ, ê³µëª¨ì „, ì½˜í…ŒìŠ¤íŠ¸ ë“±
    ]

    def __init__(self, gemini_client: Optional[GeminiClient] = None):
        """
        ë¶„ì„ê¸°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - gemini_client: Gemini í´ë¼ì´ì–¸íŠ¸ (ì—†ìœ¼ë©´ ìë™ ìƒì„±)

        ğŸ’¡ ì˜ˆì‹œ:
        analyzer = NoticeAnalyzer()  # Gemini í´ë¼ì´ì–¸íŠ¸ ìë™ ìƒì„±
        ë˜ëŠ”
        client = GeminiClient()
        analyzer = NoticeAnalyzer(gemini_client=client)  # ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ ì¬ì‚¬ìš©
        """
        # Gemini í´ë¼ì´ì–¸íŠ¸ ì„¤ì • (ì—†ìœ¼ë©´ ìƒˆë¡œ ë§Œë“¤ê¸°)
        self.client = gemini_client or GeminiClient()
        print("âœ… ê³µì§€ì‚¬í•­ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")

    def analyze_notice(self, notice_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ê³µì§€ì‚¬í•­ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - notice_data: ê³µì§€ì‚¬í•­ ë°ì´í„° (ë”•ì…”ë„ˆë¦¬ í˜•íƒœ)
          {
              "title": "ê³µì§€ì‚¬í•­ ì œëª©",
              "content": "ê³µì§€ì‚¬í•­ ë‚´ìš©",
              "url": "ê³µì§€ì‚¬í•­ ë§í¬",
              "date": "2024-01-22"
          }

        ğŸ¯ í•˜ëŠ” ì¼:
        1. ì œëª©ê³¼ ë‚´ìš©ì„ í•©ì³ì„œ ì „ì²´ í…ìŠ¤íŠ¸ ë§Œë“¤ê¸°
        2. Gemini AIë¡œ ìš”ì•½, ì¹´í…Œê³ ë¦¬, ì¤‘ìš”ë„, í‚¤ì›Œë“œ ë¶„ì„
        3. ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë”•ì…”ë„ˆë¦¬ë¡œ ì •ë¦¬

        ğŸ’¡ ì˜ˆì‹œ:
        notice = {
            "title": "ìˆ˜ê°•ì‹ ì²­ ì•ˆë‚´",
            "content": "2024ë…„ 1í•™ê¸° ìˆ˜ê°•ì‹ ì²­ì€ 2ì›” 1ì¼ë¶€í„°...",
            "url": "http://example.com",
            "date": "2024-01-20"
        }
        result = analyzer.analyze_notice(notice)
        print(result)
        # {
        #     "summary": "1í•™ê¸° ìˆ˜ê°•ì‹ ì²­ 2ì›” 1ì¼ ì‹œì‘",
        #     "category": "í•™ì‚¬",
        #     "importance": 5,
        #     "keywords": ["ìˆ˜ê°•ì‹ ì²­", "1í•™ê¸°", "2ì›” 1ì¼"],
        #     ...
        # }
        """
        # ì œëª©ê³¼ ë‚´ìš© ì¶”ì¶œ
        title = notice_data.get("title", "")
        content = notice_data.get("content", "")

        # ì „ì²´ í…ìŠ¤íŠ¸ ë§Œë“¤ê¸°
        full_text = f"ì œëª©: {title}\n\në‚´ìš©: {content}"

        print(f"ğŸ“„ ê³µì§€ì‚¬í•­ ë¶„ì„ ì‹œì‘: {title[:30]}...")

        # ê°ì¢… ë¶„ì„ ìˆ˜í–‰
        summary = self.extract_summary(full_text)
        category = self.categorize(full_text)
        keywords = self.extract_keywords(full_text)

        # ê²°ê³¼ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
        analysis_result = {
            # ì›ë³¸ ë°ì´í„°
            "original_title": title,
            "original_content": content,
            "url": notice_data.get("url", ""),
            "published_date": notice_data.get("date", ""),

            # ë¶„ì„ ê²°ê³¼
            "summary": summary,
            "category": category,
            "keywords": keywords,

            # ë©”íƒ€ ì •ë³´
            "analyzed": True,
            "analysis_model": self.client.model_name
        }

        print(f"âœ… ë¶„ì„ ì™„ë£Œ: {category}")
        return analysis_result

    def extract_summary(self, text: str, max_length: int = 100) -> str:
        """
        ê³µì§€ì‚¬í•­ì„ ìš”ì•½í•©ë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - text: ìš”ì•½í•  í…ìŠ¤íŠ¸
        - max_length: ìµœëŒ€ ìš”ì•½ ê¸¸ì´ (ê¸€ì ìˆ˜)

        ğŸ¯ í•˜ëŠ” ì¼:
        ê¸´ ê³µì§€ì‚¬í•­ì„ ì§§ê²Œ ìš”ì•½í•´ì„œ í•µì‹¬ë§Œ ì „ë‹¬í•©ë‹ˆë‹¤.

        ğŸ’¡ ì˜ˆì‹œ:
        ê¸´_ê³µì§€ = "2024í•™ë…„ë„ 1í•™ê¸° ìˆ˜ê°•ì‹ ì²­ì€ 2ì›” 1ì¼ë¶€í„° ì‹œì‘ë©ë‹ˆë‹¤. í•™ë…„ë³„ë¡œ..."
        ìš”ì•½ = analyzer.extract_summary(ê¸´_ê³µì§€)
        print(ìš”ì•½)  # "1í•™ê¸° ìˆ˜ê°•ì‹ ì²­ 2ì›” 1ì¼ ì‹œì‘, í•™ë…„ë³„ ì¼ì • í™•ì¸ í•„ìš”"
        """
        prompt = f"""
        ë‹¤ìŒ ê³µì§€ì‚¬í•­ì„ {max_length}ì ì´ë‚´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
        í•µì‹¬ ë‚´ìš©ë§Œ ê°„ê²°í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

        ê³µì§€ì‚¬í•­:
        {text}

        ìš”ì•½ ({max_length}ì ì´ë‚´):
        """

        summary = self.client.generate_text(prompt, temperature=0.3)
        return summary.strip()

    def categorize(self, text: str) -> str:
        """
        ê³µì§€ì‚¬í•­ì˜ ì¹´í…Œê³ ë¦¬ë¥¼ íŒë‹¨í•©ë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - text: ë¶„ë¥˜í•  ê³µì§€ì‚¬í•­ í…ìŠ¤íŠ¸

        ğŸ¯ í•˜ëŠ” ì¼:
        ê³µì§€ì‚¬í•­ì´ í•™ì‚¬/ì¥í•™/ì·¨ì—…/í–‰ì‚¬/êµìœ¡/ê³µëª¨ì „ ì¤‘ ì–´ë””ì— ì†í•˜ëŠ”ì§€ íŒë‹¨í•©ë‹ˆë‹¤.

        ğŸ’¡ ì˜ˆì‹œ:
        ê³µì§€ = "2024ë…„ 1í•™ê¸° ìˆ˜ê°•ì‹ ì²­ ì•ˆë‚´..."
        ì¹´í…Œê³ ë¦¬ = analyzer.categorize(ê³µì§€)
        print(ì¹´í…Œê³ ë¦¬)  # "í•™ì‚¬"
        """
        categories_str = ", ".join(self.CATEGORIES)

        prompt = f"""
        ë‹¤ìŒ ê³µì§€ì‚¬í•­ì„ ì•„ë˜ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•´ì£¼ì„¸ìš”.
        ì¹´í…Œê³ ë¦¬: {categories_str}

        ì¹´í…Œê³ ë¦¬ ì´ë¦„ë§Œ ì •í™•íˆ ë‹µí•´ì£¼ì„¸ìš”. (ì˜ˆ: í•™ì‚¬)

        ê³µì§€ì‚¬í•­:
        {text}

        ì¹´í…Œê³ ë¦¬:
        """

        category = self.client.generate_text(prompt, temperature=0.1)  # ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ì€ temperature
        category = category.strip()

        # ì¹´í…Œê³ ë¦¬ ëª©ë¡ì— ì—†ìœ¼ë©´ "í•™ì‚¬"ë¡œ ì²˜ë¦¬ (ê¸°ë³¸ê°’)
        if category not in self.CATEGORIES:
            print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì¹´í…Œê³ ë¦¬ '{category}' -> 'í•™ì‚¬'ë¡œ ë³€ê²½")
            category = "í•™ì‚¬"

        return category


    def extract_keywords(self, text: str, max_keywords: int = 5) -> List[str]:
        """
        ê³µì§€ì‚¬í•­ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - text: í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  í…ìŠ¤íŠ¸
        - max_keywords: ìµœëŒ€ í‚¤ì›Œë“œ ê°œìˆ˜

        ğŸ¯ í•˜ëŠ” ì¼:
        ê³µì§€ì‚¬í•­ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë‹¨ì–´ë“¤ì„ ì°¾ì•„ëƒ…ë‹ˆë‹¤.

        ğŸ’¡ ì˜ˆì‹œ:
        ê³µì§€ = "2024ë…„ 1í•™ê¸° ìˆ˜ê°•ì‹ ì²­ì€ 2ì›” 1ì¼ë¶€í„° ì‹œì‘ë©ë‹ˆë‹¤."
        í‚¤ì›Œë“œ = analyzer.extract_keywords(ê³µì§€)
        print(í‚¤ì›Œë“œ)  # ["ìˆ˜ê°•ì‹ ì²­", "1í•™ê¸°", "2ì›” 1ì¼", "2024ë…„"]
        """
        prompt = f"""
        ë‹¤ìŒ ê³µì§€ì‚¬í•­ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ìµœëŒ€ {max_keywords}ê°œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
        ê°€ì¥ ì¤‘ìš”í•œ ë‹¨ì–´ë§Œ ë½‘ì•„ì£¼ì„¸ìš”.

        í‚¤ì›Œë“œëŠ” ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„í•´ì„œ ë‚˜ì—´í•´ì£¼ì„¸ìš”.
        ì˜ˆ: ìˆ˜ê°•ì‹ ì²­, 1í•™ê¸°, 2ì›” 1ì¼

        ê³µì§€ì‚¬í•­:
        {text}

        í•µì‹¬ í‚¤ì›Œë“œ:
        """

        keywords_str = self.client.generate_text(prompt, temperature=0.3)

        # ì‰¼í‘œë¡œ ë¶„ë¦¬í•˜ê³  ì•ë’¤ ê³µë°± ì œê±°
        keywords = [kw.strip() for kw in keywords_str.split(",")]

        # ë¹ˆ í‚¤ì›Œë“œ ì œê±° ë° ê°œìˆ˜ ì œí•œ
        keywords = [kw for kw in keywords if kw][:max_keywords]

        return keywords

    def batch_analyze(self, notices: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ì—¬ëŸ¬ ê³µì§€ì‚¬í•­ì„ í•œë²ˆì— ë¶„ì„í•©ë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - notices: ê³µì§€ì‚¬í•­ ë¦¬ìŠ¤íŠ¸ (ê°ê° ë”•ì…”ë„ˆë¦¬ í˜•íƒœ)

        ğŸ¯ í•˜ëŠ” ì¼:
        ì—¬ëŸ¬ ê°œì˜ ê³µì§€ì‚¬í•­ì„ ìˆœì„œëŒ€ë¡œ ë¶„ì„í•´ì„œ ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ëŒë ¤ì¤ë‹ˆë‹¤.

        ğŸ’¡ ì˜ˆì‹œ:
        ê³µì§€ë“¤ = [
            {"title": "ìˆ˜ê°•ì‹ ì²­", "content": "..."},
            {"title": "ì¥í•™ê¸ˆ", "content": "..."},
        ]
        ê²°ê³¼ë“¤ = analyzer.batch_analyze(ê³µì§€ë“¤)
        for ê²°ê³¼ in ê²°ê³¼ë“¤:
            print(ê²°ê³¼["summary"])
        """
        print(f"ğŸ“š {len(notices)}ê°œ ê³µì§€ì‚¬í•­ ì¼ê´„ ë¶„ì„ ì‹œì‘...")

        results = []
        for i, notice in enumerate(notices, 1):
            print(f"\n[{i}/{len(notices)}] ë¶„ì„ ì¤‘...")
            try:
                result = self.analyze_notice(notice)
                results.append(result)
            except Exception as e:
                print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
                # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                results.append({
                    **notice,
                    "analyzed": False,
                    "error": str(e)
                })

        print(f"\nâœ… ì¼ê´„ ë¶„ì„ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
        return results

    def analyze_notice_comprehensive(self, notice_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ê³µì§€ì‚¬í•­ì„ í•œ ë²ˆì˜ AI í˜¸ì¶œë¡œ ì¢…í•© ë¶„ì„í•©ë‹ˆë‹¤. (TODO ìš”êµ¬ì‚¬í•­ ì¤€ìˆ˜)

        ğŸ¯ ëª©ì :
        prompts.pyì˜ êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ ë°›ìŠµë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - notice_data: ê³µì§€ì‚¬í•­ ë°ì´í„°
          {
              "title": "ì œëª©",
              "content": "ë‚´ìš©",
              "url": "ë§í¬",
              "date": "ë°œí‘œì¼"
          }

        ğŸ“Š ë°˜í™˜ê°’:
        {
            "summary": "ìš”ì•½",
            "dates": {
                "start_date": "YYYY-MM-DD",
                "end_date": "YYYY-MM-DD",
                "deadline": "YYYY-MM-DD"
            },
            "category": "ì¹´í…Œê³ ë¦¬",
            "analyzed": True,
            "analysis_model": "gemini-1.5-flash"
        }

        ğŸ’¡ íŠ¹ì§•:
        - ì¬ì‹œë„ ë¡œì§ í¬í•¨ (ìµœëŒ€ 3íšŒ, exponential backoff)
        - ë‚ ì§œ í˜•ì‹ ì •ê·œí™” (í•œê¸€ ë‚ ì§œ â†’ ISO 8601)
        - JSON ì‘ë‹µ íŒŒì‹± ë° ê²€ì¦
        """
        title = notice_data.get("title", "")
        content = notice_data.get("content", "")
        ocr_text = notice_data.get("_ocr_text", "")
        content_images = notice_data.get("content_images", [])

        # AI ë¶„ì„ìš© í…ìŠ¤íŠ¸ ì¡°í•© (OCR í…ìŠ¤íŠ¸ëŠ” ì°¸ê³  ì •ë³´ë¡œë§Œ ì „ë‹¬)
        full_text = f"ì œëª©: {title}\n\në‚´ìš©: {content}"
        if ocr_text:
            full_text += f"\n\n[ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ (ì°¸ê³ ìš©)]\n{ocr_text}"

        print(f"ğŸ“„ [ì¢…í•© ë¶„ì„] ì‹œì‘: {title[:30]}...")

        # í”„ë¡¬í”„íŠ¸ ìƒì„± (ì´ë¯¸ì§€ ì •ë³´ í¬í•¨)
        prompt = prompts.get_comprehensive_analysis_prompt(
            full_text,
            has_images=len(content_images) > 0,
            image_count=len(content_images)
        )
        config = prompts.get_prompt_config("comprehensive")

        # ì¬ì‹œë„ ë¡œì§ìœ¼ë¡œ AI í˜¸ì¶œ
        try:
            response = self._retry_with_backoff(
                lambda: self.client.generate_text(
                    prompt,
                    temperature=config["temperature"],
                    max_tokens=config["max_tokens"]
                ),
                max_retries=3
            )

            # JSON íŒŒì‹±
            parsed_result = self._parse_json_response(response)

            # ë‚ ì§œ ì •ê·œí™”
            if "dates" in parsed_result and isinstance(parsed_result["dates"], dict):
                parsed_result["dates"] = self._normalize_dates(parsed_result["dates"])

            # display_mode ìœ íš¨ì„± ê²€ì¦
            valid_display_modes = {"POSTER", "DOCUMENT", "HYBRID"}
            display_mode = parsed_result.get("display_mode", "DOCUMENT")
            if display_mode not in valid_display_modes:
                display_mode = "DOCUMENT"

            # ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ê°•ì œë¡œ DOCUMENT ëª¨ë“œ
            if not content_images:
                display_mode = "DOCUMENT"
                has_important_image = False
            else:
                has_important_image = parsed_result.get("has_important_image", False)

            # ê²°ê³¼ êµ¬ì¡°í™”
            analysis_result = {
                # ì›ë³¸ ë°ì´í„° (DB ì €ì¥ìš© í•„ë“œëª… ìœ ì§€)
                "title": title,
                "content": content,
                "original_title": title,
                "original_content": content,
                "url": notice_data.get("url") or notice_data.get("source_url", ""),
                "source_url": notice_data.get("source_url") or notice_data.get("url", ""),
                "published_date": notice_data.get("date") or notice_data.get("published_at", ""),

                # ë¶„ì„ ê²°ê³¼
                "summary": parsed_result.get("summary", ""),
                "dates": parsed_result.get("dates", {}),
                "category": parsed_result.get("category", "í•™ì‚¬"),
                "display_mode": display_mode,
                "has_important_image": has_important_image,

                # ë©”íƒ€ ì •ë³´
                "analyzed": True,
                "analysis_model": self.client.model_name,
                "analysis_timestamp": datetime.now().isoformat()
            }

            # í¬ë¡¤ëŸ¬ì—ì„œ ì „ë‹¬ëœ ì¶”ê°€ í•„ë“œ ìœ ì§€
            for field in ["original_id", "author", "views", "attachments",
                          "source_board", "board_seq", "content_images"]:
                if field in notice_data:
                    analysis_result[field] = notice_data[field]

            print(f"âœ… ë¶„ì„ ì™„ë£Œ: {analysis_result['category']}")
            return analysis_result

        except Exception as e:
            print(f"âŒ ì¢…í•© ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ êµ¬ì¡° ë°˜í™˜ (DB ì €ì¥ìš© í•„ë“œëª… ìœ ì§€)
            fallback_result = {
                "title": title,
                "content": content,
                "original_title": title,
                "original_content": content,
                "url": notice_data.get("url") or notice_data.get("source_url", ""),
                "source_url": notice_data.get("source_url") or notice_data.get("url", ""),
                "published_date": notice_data.get("date") or notice_data.get("published_at", ""),
                "summary": title[:200] if title else "",
                "dates": {},
                "category": "í•™ì‚¬",
                "display_mode": "DOCUMENT",
                "has_important_image": False,
                "analyzed": False,
                "error": str(e)
            }

            # í¬ë¡¤ëŸ¬ì—ì„œ ì „ë‹¬ëœ ì¶”ê°€ í•„ë“œ ìœ ì§€
            for field in ["original_id", "author", "views", "attachments",
                          "source_board", "board_seq", "content_images"]:
                if field in notice_data:
                    fallback_result[field] = notice_data[field]

            return fallback_result

    def _retry_with_backoff(self, func, max_retries: int = 3, initial_delay: float = 1.0):
        """
        ì¬ì‹œë„ ë¡œì§ì„ ì ìš©í•˜ì—¬ í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤. (Exponential Backoff)

        ğŸ¯ ëª©ì :
        API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ ì¬ì‹œë„í•˜ì—¬ ì•ˆì •ì„±ì„ ë†’ì…ë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - func: ì‹¤í–‰í•  í•¨ìˆ˜
        - max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸ê°’: 3)
        - initial_delay: ì´ˆê¸° ëŒ€ê¸° ì‹œê°„ (ê¸°ë³¸ê°’: 1.0ì´ˆ)

        ğŸ“Š Exponential Backoff:
        - 1íšŒ ì‹¤íŒ¨: 1ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
        - 2íšŒ ì‹¤íŒ¨: 2ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
        - 3íšŒ ì‹¤íŒ¨: 4ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
        - 3íšŒ ëª¨ë‘ ì‹¤íŒ¨: ì—ëŸ¬ ë°œìƒ

        ğŸ’¡ ì˜ˆì‹œ:
        result = self._retry_with_backoff(
            lambda: self.client.generate_text("ì§ˆë¬¸"),
            max_retries=3
        )
        """
        delay = initial_delay
        last_exception = None

        for attempt in range(max_retries):
            try:
                return func()
            except Exception as e:
                last_exception = e
                if attempt < max_retries - 1:
                    print(f"âš ï¸ ì‹œë„ {attempt + 1}/{max_retries} ì‹¤íŒ¨, {delay}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(delay)
                    delay *= 2  # Exponential backoff
                else:
                    print(f"âŒ {max_retries}íšŒ ëª¨ë‘ ì‹¤íŒ¨")

        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
        raise last_exception

    def _parse_json_response(self, response: str) -> Dict[str, Any]:
        """
        AI ì‘ë‹µì—ì„œ JSONì„ ì¶”ì¶œí•˜ê³  íŒŒì‹±í•©ë‹ˆë‹¤.

        ğŸ¯ ëª©ì :
        AIê°€ ë°˜í™˜í•œ í…ìŠ¤íŠ¸ì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œí•˜ì—¬ íŒŒì‹±í•©ë‹ˆë‹¤.

        ğŸ”§ ì²˜ë¦¬ ê³¼ì •:
        1. ```json ... ``` ì½”ë“œ ë¸”ë¡ ì œê±°
        2. ì•ë’¤ ê³µë°± ì œê±°
        3. JSON íŒŒì‹±
        4. ìœ íš¨ì„± ê²€ì¦

        ğŸ’¡ ì˜ˆì‹œ:
        response = "```json\n{\"summary\": \"ìš”ì•½\"}\n```"
        parsed = self._parse_json_response(response)
        print(parsed)  # {"summary": "ìš”ì•½"}
        """
        # JSON ì½”ë“œ ë¸”ë¡ ì œê±°
        response = response.strip()
        if response.startswith("```json"):
            response = response[7:]
        if response.startswith("```"):
            response = response[3:]
        if response.endswith("```"):
            response = response[:-3]
        response = response.strip()

        try:
            parsed = json.loads(response)
            return parsed
        except json.JSONDecodeError as e:
            print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            print(f"ì‘ë‹µ ë‚´ìš©: {response[:200]}...")
            raise ValueError(f"JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")

    def analyze_images(
        self,
        image_urls: List[str],
        title: str = "",
        base_url: str = "https://www.kunsan.ac.kr"
    ) -> str:
        """
        ì´ë¯¸ì§€ URLë“¤ì„ Gemini Visionìœ¼ë¡œ ë¶„ì„í•˜ì—¬ í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

        ğŸ¯ ëª©ì :
        ê³µì§€ì‚¬í•­ì´ ì´ë¯¸ì§€ë¡œë§Œ êµ¬ì„±ëœ ê²½ìš°, ì´ë¯¸ì§€ ë‚´ìš©ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - image_urls: ë¶„ì„í•  ì´ë¯¸ì§€ URL ë¦¬ìŠ¤íŠ¸
        - title: ê³µì§€ì‚¬í•­ ì œëª© (ì»¨í…ìŠ¤íŠ¸ ì œê³µìš©)
        - base_url: ìƒëŒ€ ê²½ë¡œ ë³€í™˜ìš© ê¸°ë³¸ URL

        ğŸ“Š ë°˜í™˜ê°’:
        - ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ ë‚´ìš©

        ğŸ’¡ ì˜ˆì‹œ:
        urls = ["/upload_data/editor/BBS_0000010/177034525863919.jpg"]
        content = analyzer.analyze_images(urls, title="ì¥í•™ê¸ˆ ì•ˆë‚´")
        print(content)  # "2026í•™ë…„ë„ êµ­ê°€ì¥í•™ê¸ˆ ì‹ ì²­ ì•ˆë‚´..."
        """
        if not image_urls:
            return ""

        print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘: {len(image_urls)}ê°œ ì´ë¯¸ì§€")

        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° PIL Imageë¡œ ë³€í™˜
        images = []
        for url in image_urls[:5]:  # ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ ì²˜ë¦¬ (ë¹„ìš© ì ˆê°)
            try:
                # ìƒëŒ€ ê²½ë¡œë©´ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                if not url.startswith("http"):
                    url = base_url + url

                # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
                response = requests.get(url, timeout=10)
                response.raise_for_status()

                # PIL Imageë¡œ ë³€í™˜
                img = Image.open(BytesIO(response.content))
                images.append(img)
                print(f"  âœ… ì´ë¯¸ì§€ ë¡œë“œ ì„±ê³µ: {url[-30:]}")

            except Exception as e:
                print(f"  âš ï¸ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {url[-30:]} ({str(e)})")
                continue

        if not images:
            print("  âŒ ë¡œë“œëœ ì´ë¯¸ì§€ ì—†ìŒ")
            return ""

        # Gemini Vision ëª¨ë¸ë¡œ ì´ë¯¸ì§€ ë¶„ì„
        try:
            # Gemini 2.0 Flash ëª¨ë¸ ì‚¬ìš© (Vision ì§€ì›)
            vision_model = genai.GenerativeModel("models/gemini-2.0-flash")

            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"""
ë‹¤ìŒì€ ëŒ€í•™êµ ê³µì§€ì‚¬í•­ì— í¬í•¨ëœ ì´ë¯¸ì§€ì…ë‹ˆë‹¤.
ê³µì§€ì‚¬í•­ ì œëª©: {title}

ì´ë¯¸ì§€ì—ì„œ ëª¨ë“  í…ìŠ¤íŠ¸ ë‚´ìš©ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
í‘œ, ëª©ë¡, ë‚ ì§œ, ì—°ë½ì²˜ ë“± ì¤‘ìš”í•œ ì •ë³´ë¥¼ ë¹ ì§ì—†ì´ í¬í•¨í•´ì£¼ì„¸ìš”.
ì¶”ì¶œëœ ë‚´ìš©ë§Œ ì‘ì„±í•˜ê³ , ì„¤ëª…ì´ë‚˜ í•´ì„ì€ í•˜ì§€ ë§ˆì„¸ìš”.

ì¶”ì¶œëœ ë‚´ìš©:
"""

            # ì´ë¯¸ì§€ì™€ í•¨ê»˜ ìš”ì²­
            content_parts = [prompt] + images
            response = vision_model.generate_content(
                content_parts,
                generation_config={
                    "max_output_tokens": 4096,
                    "temperature": 0.1
                }
            )

            extracted_text = response.text.strip()
            print(f"  âœ… ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ: {len(extracted_text)}ì ì¶”ì¶œ")

            return extracted_text

        except Exception as e:
            print(f"  âŒ ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return ""

    def _normalize_dates(self, dates: Dict[str, Any]) -> Dict[str, Optional[str]]:
        """
        ë‚ ì§œ ì •ë³´ë¥¼ ISO 8601 í˜•ì‹(YYYY-MM-DD)ìœ¼ë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤.

        ğŸ¯ ëª©ì :
        ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë‚ ì§œë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

        ğŸ”§ ì²˜ë¦¬ ê°€ëŠ¥í•œ í˜•ì‹:
        - YYYY-MM-DD (ì´ë¯¸ í‘œì¤€ í˜•ì‹)
        - YYYY/MM/DD
        - YYYY.MM.DD
        - null, None, "null" â†’ None
        - ë¹ˆ ë¬¸ìì—´ â†’ None

        ğŸ’¡ ì˜ˆì‹œ:
        dates = {
            "start_date": "2024/02/01",
            "end_date": "2024.02.05",
            "deadline": null
        }
        normalized = self._normalize_dates(dates)
        print(normalized)
        # {
        #     "start_date": "2024-02-01",
        #     "end_date": "2024-02-05",
        #     "deadline": None
        # }
        """
        normalized = {}

        for key, value in dates.items():
            # date_typeì€ ë‚ ì§œê°€ ì•„ë‹Œ ë¶„ë¥˜ í•„ë“œì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ìœ ì§€
            if key == "date_type":
                normalized[key] = value
                continue

            # null ê°’ ì²˜ë¦¬
            if value is None or value == "null" or value == "":
                normalized[key] = None
                continue

            # ë¬¸ìì—´ì¸ ê²½ìš° ì •ê·œí™”
            if isinstance(value, str):
                # ì´ë¯¸ YYYY-MM-DD í˜•ì‹ì¸ì§€ í™•ì¸
                if re.match(r'^\d{4}-\d{2}-\d{2}$', value):
                    normalized[key] = value
                # YYYY/MM/DD í˜•ì‹
                elif re.match(r'^\d{4}/\d{2}/\d{2}$', value):
                    normalized[key] = value.replace("/", "-")
                # YYYY.MM.DD í˜•ì‹
                elif re.match(r'^\d{4}\.\d{2}\.\d{2}$', value):
                    normalized[key] = value.replace(".", "-")
                else:
                    # í˜•ì‹ì´ ë§ì§€ ì•Šìœ¼ë©´ ì›ë³¸ ìœ ì§€
                    print(f"âš ï¸ ë‚ ì§œ í˜•ì‹ ë¶ˆì¼ì¹˜: {key}={value}")
                    normalized[key] = value
            else:
                normalized[key] = value

        return normalized


# ğŸ§ª í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    print("=" * 50)
    print("ğŸ§ª ê³µì§€ì‚¬í•­ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 50)

    try:
        # 1. ë¶„ì„ê¸° ìƒì„±
        print("\n[1ë‹¨ê³„] ë¶„ì„ê¸° ì´ˆê¸°í™” ì¤‘...")
        analyzer = NoticeAnalyzer()

        # 2. í…ŒìŠ¤íŠ¸ ê³µì§€ì‚¬í•­
        test_notice = {
            "title": "[í•™ì‚¬ê³µì§€] 2024í•™ë…„ë„ 1í•™ê¸° ìˆ˜ê°•ì‹ ì²­ ì•ˆë‚´",
            "content": """
            ìˆ˜ê°•ì‹ ì²­ ì¼ì •ì„ ë‹¤ìŒê³¼ ê°™ì´ ì•ˆë‚´í•©ë‹ˆë‹¤.

            1. ìˆ˜ê°•ì‹ ì²­ ê¸°ê°„
               - 4í•™ë…„: 2024ë…„ 2ì›” 1ì¼ 10:00 ~ 2ì›” 2ì¼ 18:00
               - 3í•™ë…„: 2024ë…„ 2ì›” 2ì¼ 10:00 ~ 2ì›” 3ì¼ 18:00
               - 2í•™ë…„: 2024ë…„ 2ì›” 3ì¼ 10:00 ~ 2ì›” 4ì¼ 18:00
               - 1í•™ë…„: 2024ë…„ 2ì›” 4ì¼ 10:00 ~ 2ì›” 5ì¼ 18:00

            2. ìˆ˜ê°•ì‹ ì²­ ë°©ë²•
               - í•™êµ í¬í„¸ ì ‘ì† í›„ 'ìˆ˜ê°•ì‹ ì²­' ë©”ë‰´ ì´ìš©
               - ìµœëŒ€ 21í•™ì ê¹Œì§€ ì‹ ì²­ ê°€ëŠ¥

            3. ì£¼ì˜ì‚¬í•­
               - ì„ ìˆ˜ê³¼ëª© ì´ìˆ˜ ì—¬ë¶€ í™•ì¸ í•„ìˆ˜
               - ì‹œê°„í‘œ ì¤‘ë³µ í™•ì¸

            í•™ìƒì§€ì›ì²˜ í•™ì‚¬ìš´ì˜íŒ€
            """,
            "url": "https://kunsan.ac.kr/notice/123",
            "date": "2024-01-20"
        }

        # 3. ì¢…í•© ë¶„ì„
        print("\n[2ë‹¨ê³„] ê³µì§€ì‚¬í•­ ì¢…í•© ë¶„ì„ ì¤‘...")
        result = analyzer.analyze_notice(test_notice)

        print("\nğŸ“Š ë¶„ì„ ê²°ê³¼:")
        print(f"  ğŸ“ ìš”ì•½: {result['summary']}")
        print(f"  ğŸ·ï¸ ì¹´í…Œê³ ë¦¬: {result['category']}")
        print(f"  â­ ì¤‘ìš”ë„: {result['importance']}ì ")
        print(f"  ğŸ”‘ í‚¤ì›Œë“œ: {', '.join(result['keywords'])}")

        # 4. ê°œë³„ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
        print("\n[3ë‹¨ê³„] ê°œë³„ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸...")

        print("\n  ğŸ“ ìš”ì•½ë§Œ ì¶”ì¶œ:")
        summary = analyzer.extract_summary(test_notice["content"])
        print(f"  {summary}")

        print("\n  ğŸ·ï¸ ì¹´í…Œê³ ë¦¬ë§Œ ë¶„ë¥˜:")
        category = analyzer.categorize(test_notice["content"])
        print(f"  {category}")

        print("\n  â­ ì¤‘ìš”ë„ë§Œ í‰ê°€:")
        importance = analyzer.calculate_importance(test_notice["content"])
        print(f"  {importance}ì ")

        print("\n  ğŸ”‘ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ:")
        keywords = analyzer.extract_keywords(test_notice["content"])
        print(f"  {', '.join(keywords)}")

        print("\n" + "=" * 50)
        print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("=" * 50)

    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
