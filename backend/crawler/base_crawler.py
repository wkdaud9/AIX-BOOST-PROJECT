# -*- coding: utf-8 -*-
"""
ê¸°ë³¸ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤

ğŸ¤” ì´ íŒŒì¼ì´ í•˜ëŠ” ì¼:
ëª¨ë“  í¬ë¡¤ëŸ¬ê°€ ê³µí†µìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê¸°ë³¸ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
ê³µì§€ì‚¬í•­, í•™ì‚¬ì¥í•™, ëª¨ì§‘ê³µê³  í¬ë¡¤ëŸ¬ê°€ ì´ í´ë˜ìŠ¤ë¥¼ ìƒì†ë°›ì•„ ì‚¬ìš©í•©ë‹ˆë‹¤.

ğŸ“š ë¹„ìœ :
- ì´ í´ë˜ìŠ¤ = ëª¨ë“  ì°¨(ìë™ì°¨)ì˜ ê¸°ë³¸ ì„¤ê³„ë„
- ìì‹ í¬ë¡¤ëŸ¬ë“¤ = ì´ ì„¤ê³„ë„ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë§Œë“  ìŠ¹ìš©ì°¨, íŠ¸ëŸ­, ë²„ìŠ¤
"""

import requests
from bs4 import BeautifulSoup
from typing import List, Dict, Any, Optional
from datetime import datetime
import time
import re
import random


class BaseCrawler:
    """
    ëª¨ë“  í¬ë¡¤ëŸ¬ì˜ ë¶€ëª¨ í´ë˜ìŠ¤

    ğŸ¯ ëª©ì :
    ì›¹ í˜ì´ì§€ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ê¸°ë³¸ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

    ğŸ—ï¸ ì£¼ìš” ê¸°ëŠ¥:
    1. fetch_page: ì›¹ í˜ì´ì§€ HTML ê°€ì ¸ì˜¤ê¸°
    2. parse_date: ë‚ ì§œ ë¬¸ìì—´ì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    3. clean_text: í…ìŠ¤íŠ¸ ì •ë¦¬ (ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì œê±°)
    4. crawl: í¬ë¡¤ë§ ì‹¤í–‰ (ìì‹ í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„)
    """

    def __init__(self, base_url: str, category: str):
        """
        í¬ë¡¤ëŸ¬ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - base_url: í¬ë¡¤ë§í•  ì›¹ì‚¬ì´íŠ¸ ì£¼ì†Œ
        - category: ì¹´í…Œê³ ë¦¬ ì´ë¦„ (ì˜ˆ: "ê³µì§€ì‚¬í•­", "í•™ì‚¬ì¥í•™")

        ğŸ’¡ ì˜ˆì‹œ:
        crawler = BaseCrawler(
            base_url="https://www.kunsan.ac.kr/board/list.kunsan",
            category="ê³µì§€ì‚¬í•­"
        )
        """
        self.base_url = base_url
        self.category = category
        self.session = requests.Session()

        # HTTP ìš”ì²­ í—¤ë” (ì‚¬ëŒì²˜ëŸ¼ ë³´ì´ê²Œ í•˜ê¸°)
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
        })

        print(f"[OK] {category} í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")

    def fetch_page(
        self,
        url: str,
        params: Optional[Dict] = None,
        max_retries: int = 3,
        delay_range: tuple = (1.0, 2.0)
    ) -> Optional[BeautifulSoup]:
        """
        ì›¹ í˜ì´ì§€ì˜ HTMLì„ ê°€ì ¸ì˜µë‹ˆë‹¤. (ì¬ì‹œë„ ë° ë°±ì˜¤í”„ ë¡œì§ í¬í•¨)

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - url: ê°€ì ¸ì˜¬ í˜ì´ì§€ ì£¼ì†Œ
        - params: URL íŒŒë¼ë¯¸í„° (ë”•ì…”ë„ˆë¦¬)
        - max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸ê°’: 3)
        - delay_range: ìš”ì²­ ê°„ ë”œë ˆì´ ë²”ìœ„ (ì´ˆ ë‹¨ìœ„, ê¸°ë³¸ê°’: 1~2ì´ˆ)

        ğŸ¯ í•˜ëŠ” ì¼:
        1. requestsë¡œ ì›¹ í˜ì´ì§€ ìš”ì²­
        2. HTMLì„ BeautifulSoupìœ¼ë¡œ íŒŒì‹±
        3. ì‹¤íŒ¨ ì‹œ exponential backoffë¡œ ì¬ì‹œë„
        4. íŒŒì‹±ëœ ê°ì²´ ë°˜í™˜

        ğŸ’¡ ì˜ˆì‹œ:
        soup = crawler.fetch_page("https://example.com")
        ì œëª© = soup.find("h1").text
        """
        for attempt in range(1, max_retries + 1):
            try:
                if attempt == 1:
                    print(f"[í˜ì´ì§€] í˜ì´ì§€ ìš”ì²­ ì¤‘: {url}")
                else:
                    print(f"[ì¬ì‹œë„ {attempt}/{max_retries}] {url}")

                # ì›¹ í˜ì´ì§€ ìš”ì²­
                response = self.session.get(url, params=params, timeout=10)
                response.raise_for_status()  # ì—ëŸ¬ í™•ì¸

                # ì¸ì½”ë”© ì„¤ì • (í•œê¸€ ê¹¨ì§ ë°©ì§€)
                response.encoding = 'utf-8'

                # HTML íŒŒì‹±
                soup = BeautifulSoup(response.text, 'html.parser')

                # ì„œë²„ ë¶€ë‹´ ìµœì†Œí™”: 1~2ì´ˆ ëœë¤ ëŒ€ê¸°
                delay = random.uniform(*delay_range)
                time.sleep(delay)

                return soup

            except requests.exceptions.Timeout:
                print(f"[íƒ€ì„ì•„ì›ƒ] {url}")
                if attempt < max_retries:
                    # Exponential backoff (2^attempt ì´ˆ)
                    backoff_time = 2 ** attempt
                    print(f"  â†’ {backoff_time}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(backoff_time)
                else:
                    return None

            except requests.exceptions.RequestException as e:
                print(f"[ERROR] í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {str(e)}")
                if attempt < max_retries:
                    # Exponential backoff
                    backoff_time = 2 ** attempt
                    print(f"  â†’ {backoff_time}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(backoff_time)
                else:
                    return None

        return None

    def parse_date(self, date_str: str) -> Optional[datetime]:
        """
        ë‚ ì§œ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - date_str: ë‚ ì§œ ë¬¸ìì—´ (ì˜ˆ: "2024-01-22", "2024.01.22")

        ğŸ¯ í•˜ëŠ” ì¼:
        ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë‚ ì§œë¥¼ í‘œì¤€ datetimeìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

        ğŸ’¡ ì˜ˆì‹œ:
        ë‚ ì§œ1 = crawler.parse_date("2024-01-22")
        ë‚ ì§œ2 = crawler.parse_date("2024.01.22")
        ë‚ ì§œ3 = crawler.parse_date("24-01-22")
        # ëª¨ë‘ ê°™ì€ datetime ê°ì²´ë¡œ ë³€í™˜ë¨
        """
        if not date_str:
            return None

        # ê³µë°± ì œê±°
        date_str = date_str.strip()

        # ì‹œë„í•  ë‚ ì§œ í˜•ì‹ë“¤
        date_formats = [
            "%Y-%m-%d",           # 2024-01-22
            "%Y.%m.%d",           # 2024.01.22
            "%Y/%m/%d",           # 2024/01/22
            "%y-%m-%d",           # 24-01-22
            "%y.%m.%d",           # 24.01.22
            "%Y-%m-%d %H:%M:%S",  # 2024-01-22 14:30:00
            "%Y.%m.%d %H:%M",     # 2024.01.22 14:30
        ]

        for fmt in date_formats:
            try:
                return datetime.strptime(date_str, fmt)
            except ValueError:
                continue

        print(f"[WARNING] ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {date_str}")
        return None

    def clean_text(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤ (ê³µë°±, íŠ¹ìˆ˜ë¬¸ì ì œê±°).

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - text: ì •ë¦¬í•  í…ìŠ¤íŠ¸

        ğŸ¯ í•˜ëŠ” ì¼:
        1. ì•ë’¤ ê³µë°± ì œê±°
        2. ì—¬ëŸ¬ ì¤„ë°”ê¿ˆì„ í•˜ë‚˜ë¡œ
        3. ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ

        ğŸ’¡ ì˜ˆì‹œ:
        ì§€ì €ë¶„í•œ_í…ìŠ¤íŠ¸ = "  ì•ˆë…•í•˜ì„¸ìš”\n\n\n    ë°˜ê°‘ìŠµë‹ˆë‹¤  "
        ê¹”ë”í•œ_í…ìŠ¤íŠ¸ = crawler.clean_text(ì§€ì €ë¶„í•œ_í…ìŠ¤íŠ¸)
        print(ê¹”ë”í•œ_í…ìŠ¤íŠ¸)
        # "ì•ˆë…•í•˜ì„¸ìš”\në°˜ê°‘ìŠµë‹ˆë‹¤"
        """
        if not text:
            return ""

        # ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()

        # ì—¬ëŸ¬ ì¤„ë°”ê¿ˆì„ í•˜ë‚˜ë¡œ
        text = re.sub(r'\n\s*\n', '\n', text)

        # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r' +', ' ', text)

        return text

    def extract_attachment_urls(self, soup: BeautifulSoup) -> List[str]:
        """
        ì²¨ë¶€íŒŒì¼ URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - soup: BeautifulSoup ê°ì²´

        ğŸ¯ í•˜ëŠ” ì¼:
        í˜ì´ì§€ì—ì„œ ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë§í¬ë¥¼ ì°¾ì•„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

        ğŸ’¡ ì˜ˆì‹œ:
        ì²¨ë¶€íŒŒì¼ë“¤ = crawler.extract_attachment_urls(soup)
        print(ì²¨ë¶€íŒŒì¼ë“¤)
        # ["https://example.com/file1.pdf", "https://example.com/file2.hwp"]
        """
        attachments = []

        # ì¼ë°˜ì ì¸ ì²¨ë¶€íŒŒì¼ ë§í¬ ì°¾ê¸°
        for link in soup.find_all('a', href=True):
            href = link['href']

            # íŒŒì¼ í™•ì¥ì í™•ì¸
            file_extensions = ['.pdf', '.hwp', '.doc', '.docx', '.xls', '.xlsx', '.zip']
            if any(href.lower().endswith(ext) for ext in file_extensions):
                # ìƒëŒ€ ê²½ë¡œë©´ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                if not href.startswith('http'):
                    href = requests.compat.urljoin(self.base_url, href)
                attachments.append(href)

        return attachments

    def crawl(self, max_pages: int = 1) -> List[Dict[str, Any]]:
        """
        í¬ë¡¤ë§ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

        âš ï¸ ì£¼ì˜: ì´ ë©”ì„œë“œëŠ” ìì‹ í´ë˜ìŠ¤ì—ì„œ ë°˜ë“œì‹œ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤!

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - max_pages: í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜

        ğŸ¯ ë°˜í™˜ê°’:
        í¬ë¡¤ë§í•œ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        [
            {
                "title": "ì œëª©",
                "content": "ë‚´ìš©",
                "published_at": datetime ê°ì²´,
                "source_url": "ë§í¬",
                "category": "ì¹´í…Œê³ ë¦¬"
            },
            ...
        ]
        """
        raise NotImplementedError("ìì‹ í´ë˜ìŠ¤ì—ì„œ crawl() ë©”ì„œë“œë¥¼ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤!")

    def save_to_dict(
        self,
        title: str,
        content: str,
        published_at: datetime,
        source_url: str,
        **kwargs
    ) -> Dict[str, Any]:
        """
        í¬ë¡¤ë§í•œ ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì €ì¥í•©ë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - title: ì œëª©
        - content: ë‚´ìš©
        - published_at: ì‘ì„±ì¼
        - source_url: ì›ë³¸ ë§í¬
        - **kwargs: ì¶”ê°€ ì •ë³´ (ì‘ì„±ì, ì¡°íšŒìˆ˜ ë“±)

        ğŸ¯ ë°˜í™˜ê°’:
        ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•  ìˆ˜ ìˆëŠ” í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬

        ğŸ’¡ ì˜ˆì‹œ:
        ë°ì´í„° = crawler.save_to_dict(
            title="ìˆ˜ê°•ì‹ ì²­ ì•ˆë‚´",
            content="2024ë…„ 1í•™ê¸°...",
            published_at=datetime.now(),
            source_url="https://...",
            author="í•™ìƒì§€ì›ì²˜"
        )
        """
        return {
            "title": self.clean_text(title),
            "content": self.clean_text(content),
            "source_board": self.category,  # ê²Œì‹œíŒëª… (categoryëŠ” AIê°€ íŒë³„)
            "published_at": published_at,
            "source_url": source_url,
            **kwargs  # ì¶”ê°€ ì •ë³´
        }


# ğŸ§ª í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    print("=" * 50)
    print("ğŸ§ª ê¸°ë³¸ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 50)

    # 1. í¬ë¡¤ëŸ¬ ìƒì„±
    print("\n[1ë‹¨ê³„] í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”...")
    crawler = BaseCrawler(
        base_url="https://www.kunsan.ac.kr",
        category="í…ŒìŠ¤íŠ¸"
    )

    # 2. ë‚ ì§œ íŒŒì‹± í…ŒìŠ¤íŠ¸
    print("\n[2ë‹¨ê³„] ë‚ ì§œ íŒŒì‹± í…ŒìŠ¤íŠ¸...")
    test_dates = [
        "2024-01-22",
        "2024.01.22",
        "24-01-22",
        "2024-01-22 14:30:00"
    ]

    for date_str in test_dates:
        parsed = crawler.parse_date(date_str)
        print(f"  '{date_str}' â†’ {parsed}")

    # 3. í…ìŠ¤íŠ¸ ì •ë¦¬ í…ŒìŠ¤íŠ¸
    print("\n[3ë‹¨ê³„] í…ìŠ¤íŠ¸ ì •ë¦¬ í…ŒìŠ¤íŠ¸...")
    dirty_text = "  ì•ˆë…•í•˜ì„¸ìš”\n\n\n    ë°˜ê°‘ìŠµë‹ˆë‹¤  "
    clean = crawler.clean_text(dirty_text)
    print(f"  ì •ë¦¬ ì „: {repr(dirty_text)}")
    print(f"  ì •ë¦¬ í›„: {repr(clean)}")

    # 4. ë°ì´í„° ì €ì¥ í…ŒìŠ¤íŠ¸
    print("\n[4ë‹¨ê³„] ë°ì´í„° ì €ì¥ í˜•ì‹ í…ŒìŠ¤íŠ¸...")
    data = crawler.save_to_dict(
        title="í…ŒìŠ¤íŠ¸ ì œëª©",
        content="í…ŒìŠ¤íŠ¸ ë‚´ìš©ì…ë‹ˆë‹¤.",
        published_at=datetime.now(),
        source_url="https://example.com",
        author="í…ŒìŠ¤í„°"
    )
    print(f"  ì €ì¥ ë°ì´í„°:")
    for key, value in data.items():
        print(f"    {key}: {value}")

    print("\n" + "=" * 50)
    print("âœ… ê¸°ë³¸ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 50)
