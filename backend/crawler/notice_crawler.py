# -*- coding: utf-8 -*-
"""
êµ°ì‚°ëŒ€í•™êµ ê³µì§€ì‚¬í•­ í¬ë¡¤ëŸ¬

ğŸ¤” ì´ íŒŒì¼ì´ í•˜ëŠ” ì¼:
êµ°ì‚°ëŒ€í•™êµ í™ˆí˜ì´ì§€ì˜ ê³µì§€ì‚¬í•­ ê²Œì‹œíŒì—ì„œ ìµœì‹  ê³µì§€ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.

ğŸ“š ë¹„ìœ :
- í•™êµ ê²Œì‹œíŒ = ê³µì§€ì‚¬í•­ì´ ë¶™ì–´ìˆëŠ” í° ê²Œì‹œíŒ
- ì´ í¬ë¡¤ëŸ¬ = ê²Œì‹œíŒì„ ë³´ê³  ì¤‘ìš”í•œ ê³µì§€ë¥¼ ì‚¬ì§„ ì°ì–´ì„œ ì €ì¥í•˜ëŠ” í•™ìƒ
"""

from .base_crawler import BaseCrawler
from typing import List, Dict, Any, Optional
from datetime import datetime
from markdownify import markdownify as md, MarkdownConverter
import re
import sys
import os


def clean_markdown(text: str) -> str:
    """
    markdownify ì¶œë ¥ì˜ íŒŒí¸í™”ëœ ì„œì‹ì„ ì •ë¦¬í•©ë‹ˆë‹¤.

    í•™êµ í™ˆí˜ì´ì§€ HTMLì—ì„œ <b>2026</b><b>ë…„</b> ê°™ì´ íƒœê·¸ê°€ íŒŒí¸í™”ë˜ì–´ ìˆìœ¼ë©´
    markdownifyê°€ **2026****ë…„** ìœ¼ë¡œ ë³€í™˜í•˜ë¯€ë¡œ ì´ë¥¼ **2026ë…„** ìœ¼ë¡œ ë³‘í•©í•©ë‹ˆë‹¤.
    """
    # 1. ì—°ì†ëœ bold ë§ˆì»¤ ë³‘í•©: **text****text** â†’ **texttext**
    text = text.replace('****', '')
    # 2. ì¸ì ‘í•œ bold êµ¬ê°„ ë³‘í•©: **text** **text** â†’ **text text**
    text = re.sub(r'\*\*(\s+)\*\*', r'\1', text)
    # 3. ë¹ˆ bold ì œê±°: **  ** â†’ (ë¹ˆ ë¬¸ìì—´)
    text = re.sub(r'\*\*\s*\*\*', '', text)
    # 4. íŠ¹ìˆ˜ë¬¸ì ì£¼ìœ„ íŒŒí¸í™”ëœ bold ì œê±°: **â€»** â†’ â€», **â€§** â†’ â€§
    text = re.sub(r'\*\*([^\w\s])\*\*', r'\1', text)
    # 5. ì§ì´ ë§ì§€ ì•ŠëŠ” bold ë§ˆì»¤ ì œê±° (ì¤„ ë‹¨ìœ„ë¡œ ** ê°œìˆ˜ê°€ í™€ìˆ˜ë©´ ì œê±°)
    lines = text.split('\n')
    fixed_lines = []
    for line in lines:
        if line.count('**') % 2 != 0:
            line = line.replace('**', '')
        fixed_lines.append(line)
    text = '\n'.join(fixed_lines)
    # 6. ì´ìŠ¤ì¼€ì´í”„ëœ asterisk ë³µì›: \* â†’ * (markdownifyê°€ * ë¥¼ \* ë¡œ ì´ìŠ¤ì¼€ì´í”„)
    text = text.replace('\\*', '*')
    # 7. ì—°ì† ë¹ˆ ì¤„ ì •ë¦¬ (3ì¤„ ì´ìƒ â†’ 2ì¤„)
    text = re.sub(r'\n{3,}', '\n\n', text)
    return text.strip()


class AlignPreservingConverter(MarkdownConverter):
    """
    text-align CSS ì†ì„±ì„ ë§ˆì»¤ë¡œ ë³´ì¡´í•˜ëŠ” Markdown ë³€í™˜ê¸°

    markdownifyëŠ” ê¸°ë³¸ì ìœ¼ë¡œ inline styleì„ ë¬´ì‹œí•˜ë¯€ë¡œ text-align: center ë“±ì´ ì‚¬ë¼ì§‘ë‹ˆë‹¤.
    ì´ ë³€í™˜ê¸°ëŠ” ì •ë ¬ ì •ë³´ë¥¼ {=center=}...{=/center=} ë§ˆì»¤ë¡œ ë³´ì¡´í•˜ì—¬
    í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì •ë ¬ì„ ë³µì›í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.
    """

    def _get_align(self, el):
        """ì—˜ë¦¬ë¨¼íŠ¸ì˜ text-align ì†ì„± í™•ì¸ (CSS style + HTML align ì†ì„±)"""
        style = el.get('style', '') if hasattr(el, 'get') else ''
        align_attr = el.get('align', '') if hasattr(el, 'get') else ''

        if 'text-align' in style:
            match = re.search(r'text-align\s*:\s*(center|right)', style)
            if match:
                return match.group(1)
        if align_attr in ('center', 'right'):
            return align_attr
        return None

    def convert_p(self, el, text, *args, **kwargs):
        """p íƒœê·¸ ë³€í™˜ ì‹œ text-align ë³´ì¡´"""
        result = super().convert_p(el, text, *args, **kwargs)
        align = self._get_align(el)
        if align and result.strip():
            stripped = result.strip()
            result = f'\n\n{{={align}=}}{stripped}{{=/{align}=}}\n\n'
        return result

    def convert_div(self, el, text, *args, **kwargs):
        """div íƒœê·¸ ë³€í™˜ ì‹œ text-align ë³´ì¡´ (í•˜ìœ„ ìš”ì†Œì— ë§ˆì»¤ê°€ ì—†ëŠ” ê²½ìš°ë§Œ)"""
        result = super().convert_div(el, text, *args, **kwargs)
        align = self._get_align(el)
        # í•˜ìœ„ ìš”ì†Œì—ì„œ ì´ë¯¸ ë§ˆì»¤ê°€ ì¶”ê°€ëœ ê²½ìš° ì¤‘ë³µ ë°©ì§€
        if align and result.strip() and '{=' not in result:
            lines = result.strip().split('\n')
            marked = []
            for line in lines:
                if line.strip():
                    marked.append(f'{{={align}=}}{line}{{=/{align}=}}')
                else:
                    marked.append(line)
            result = '\n\n' + '\n'.join(marked) + '\n\n'
        return result

    def convert_center(self, el, text, *args, **kwargs):
        """<center> íƒœê·¸ë¥¼ center ë§ˆì»¤ë¡œ ë³€í™˜"""
        if text.strip():
            return f'\n\n{{=center=}}{text.strip()}{{=/center=}}\n\n'
        return text


def md_with_align(html, **kwargs):
    """text-align ë³´ì¡´ Markdown ë³€í™˜ í—¬í¼ í•¨ìˆ˜"""
    return AlignPreservingConverter(**kwargs).convert(html)


# NoticeService importë¥¼ ìœ„í•´ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(__file__)))


class NoticeCrawler(BaseCrawler):
    """
    êµ°ì‚°ëŒ€í•™êµ ê³µì§€ì‚¬í•­ í¬ë¡¤ëŸ¬

    ğŸ¯ ëª©ì :
    êµ°ì‚°ëŒ€í•™êµ í™ˆí˜ì´ì§€ì˜ ê³µì§€ì‚¬í•­ ê²Œì‹œíŒì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.

    ğŸ—ï¸ ì‘ë™ ë°©ì‹:
    1. ê³µì§€ì‚¬í•­ ëª©ë¡ í˜ì´ì§€ ì ‘ì†
    2. ê° ê³µì§€ì‚¬í•­ì˜ ì œëª©, ì‘ì„±ì¼, ë§í¬, ìˆœë²ˆ ì¶”ì¶œ
    3. DBì˜ ë§ˆì§€ë§‰ ìˆœë²ˆë³´ë‹¤ í° ê³µì§€ë§Œ ìƒì„¸ í¬ë¡¤ë§
    4. ë°ì´í„° ì •ë¦¬í•´ì„œ ë°˜í™˜
    """

    # êµ°ì‚°ëŒ€í•™êµ ê³µì§€ì‚¬í•­ URL ì„¤ì •
    BASE_URL = "https://www.kunsan.ac.kr"
    LIST_URL = "https://www.kunsan.ac.kr/board/list.kunsan"

    # URL íŒŒë¼ë¯¸í„° (ê²Œì‹œíŒ ì„¤ì •)
    BOARD_PARAMS = {
        "boardId": "BBS_0000008",
        "menuCd": "DOM_000000105001001000",
        "orderBy": "REGISTER_DATE DESC",
        "paging": "ok"
    }

    # ì›ë³¸ ê²Œì‹œíŒ ì´ë¦„ (source_board ì €ì¥ìš©)
    SOURCE_BOARD = "ê³µì§€ì‚¬í•­"

    def __init__(self):
        """
        ê³µì§€ì‚¬í•­ í¬ë¡¤ëŸ¬ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        ğŸ’¡ ì˜ˆì‹œ:
        crawler = NoticeCrawler()
        ê³µì§€ë“¤ = crawler.crawl(max_pages=3)  # ìµœëŒ€ 3í˜ì´ì§€ í¬ë¡¤ë§
        """
        super().__init__(
            base_url=self.BASE_URL,
            category="ê³µì§€ì‚¬í•­"
        )

    def check_new_notices(self, last_known_id: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        ëª©ë¡ í˜ì´ì§€ë§Œ í™•ì¸í•˜ì—¬ ìƒˆë¡œìš´ ê³µì§€ê°€ ìˆëŠ”ì§€ ì²´í¬í•©ë‹ˆë‹¤.

        ğŸ¯ ëª©ì :
        DBì˜ ë§ˆì§€ë§‰ original_idì™€ ë¹„êµí•˜ì—¬ ìƒˆ ê³µì§€ê°€ ìˆì„ ë•Œë§Œ ìƒì„¸ í¬ë¡¤ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        í•™êµ ì„œë²„ ë¶€ë‹´ì„ ìµœì†Œí™”í•˜ê³  í¬ë¡¤ë§ ì†ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - last_known_id: DBì— ì €ì¥ëœ ë§ˆì§€ë§‰ ê³µì§€ ID

        ğŸ“Š ë°˜í™˜ê°’:
        - ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ ëª©ë¡ (IDì™€ URLë§Œ í¬í•¨)

        ğŸ’¡ ì˜ˆì‹œ:
        crawler = NoticeCrawler()
        new_notices = crawler.check_new_notices(last_known_id="12345")
        if new_notices:
            print(f"{len(new_notices)}ê°œ ìƒˆ ê³µì§€ ë°œê²¬!")
        """
        print(f"\n[ì²´í¬] ìƒˆ ê³µì§€ì‚¬í•­ í™•ì¸ ì¤‘... (ë§ˆì§€ë§‰ ID: {last_known_id})")

        # 1í˜ì´ì§€ ëª©ë¡ë§Œ ê°€ì ¸ì˜¤ê¸°
        params = self.BOARD_PARAMS.copy()
        params['startPage'] = '1'

        soup = self.fetch_page(self.LIST_URL, params=params)

        if not soup:
            print("[ERROR] ëª©ë¡ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨")
            return []

        # ëª©ë¡ ì¶”ì¶œ
        notices = self._extract_notice_list(soup)

        if not notices:
            print("[INFO] ëª©ë¡ì—ì„œ ê³µì§€ì‚¬í•­ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
            return []

        # last_known_idê°€ ì—†ìœ¼ë©´ ëª¨ë“  ê³µì§€ë¥¼ ìƒˆ ê³µì§€ë¡œ ê°„ì£¼
        if not last_known_id:
            print(f"[OK] ì²« í¬ë¡¤ë§ - {len(notices)}ê°œ ëª¨ë‘ ì²˜ë¦¬")
            return notices

        # ìƒˆë¡œìš´ ê³µì§€ë§Œ í•„í„°ë§
        new_notices = []
        for notice in notices:
            notice_id = notice.get("notice_id")

            # ë§ˆì§€ë§‰ ì•Œë ¤ì§„ IDë¥¼ ë§Œë‚˜ë©´ ì¤‘ë‹¨
            if notice_id == last_known_id:
                print(f"[OK] ë§ˆì§€ë§‰ ì €ì¥ ê³µì§€ ë°œê²¬ - {len(new_notices)}ê°œ ìƒˆ ê³µì§€")
                break

            new_notices.append(notice)

        # ëª¨ë“  ê³µì§€ê°€ ìƒˆ ê³µì§€ì¸ ê²½ìš° (ë§ˆì§€ë§‰ IDë¥¼ ì°¾ì§€ ëª»í•¨)
        if len(new_notices) == len(notices):
            print(f"[WARNING] ë§ˆì§€ë§‰ IDë¥¼ ì°¾ì§€ ëª»í•¨ - ëª¨ë“  ê³µì§€ ì²˜ë¦¬ ({len(new_notices)}ê°œ)")

        return new_notices

    def crawl_optimized(
        self,
        last_board_seq: Optional[int] = None,
        max_pages: int = 1,
        max_notices: int = 10,
        **kwargs  # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ existing_urls ë“± ë¬´ì‹œ
    ) -> List[Dict[str, Any]]:
        """
        ìµœì í™”ëœ í¬ë¡¤ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. (ìˆœë²ˆ ê¸°ë°˜ ì¤‘ë³µ ì²´í¬)

        ğŸ¯ ëª©ì :
        1. DBì—ì„œ í•´ë‹¹ ê²Œì‹œíŒì˜ ë§ˆì§€ë§‰ ìˆœë²ˆ ì¡°íšŒ
        2. ëª©ë¡ í˜ì´ì§€ì—ì„œ ë§ˆì§€ë§‰ ìˆœë²ˆë³´ë‹¤ í° ê³µì§€ë§Œ í¬ë¡¤ë§
        3. ê²Œì‹œíŒë‹¹ ìµœëŒ€ max_noticesê°œê¹Œì§€ë§Œ í¬ë¡¤ë§
        4. í•™êµ ì„œë²„ ë¶€ë‹´ ìµœì†Œí™” + DB ì¡°íšŒ ë¹„ìš© ê°ì†Œ

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - last_board_seq: DBì— ì €ì¥ëœ ë§ˆì§€ë§‰ ìˆœë²ˆ (ì—†ìœ¼ë©´ ë‚´ë¶€ì—ì„œ ì¡°íšŒ)
        - max_pages: ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: 1)
        - max_notices: ê²Œì‹œíŒë‹¹ ìµœëŒ€ í¬ë¡¤ë§ ê°œìˆ˜ (ê¸°ë³¸ê°’: 10)

        ğŸ“Š ë°˜í™˜ê°’:
        - í¬ë¡¤ë§í•œ ê³µì§€ì‚¬í•­ ë¦¬ìŠ¤íŠ¸ (ìƒì„¸ ì •ë³´ í¬í•¨)

        ğŸ’¡ ì˜ˆì‹œ:
        crawler = NoticeCrawler()
        notices = crawler.crawl_optimized(max_pages=1, max_notices=10)
        print(f"ìƒˆë¡œìš´ ê³µì§€: {len(notices)}ê°œ")
        """
        print(f"\n{'='*50}")
        print(f"[ìµœì í™” í¬ë¡¤ë§] ìˆœë²ˆ ê¸°ë°˜ ì¤‘ë³µ ì²´í¬ ({self.SOURCE_BOARD})")
        print(f"{'='*50}\n")

        # ë§ˆì§€ë§‰ ìˆœë²ˆì´ ì—†ìœ¼ë©´ DBì—ì„œ ì¡°íšŒ
        if last_board_seq is None:
            last_board_seq = self._get_last_board_seq()

        print(f"[ì •ë³´] DB ë§ˆì§€ë§‰ ìˆœë²ˆ: {last_board_seq if last_board_seq else 'ì—†ìŒ (ì²« í¬ë¡¤ë§)'}")
        print(f"[ì •ë³´] ìµœëŒ€ í¬ë¡¤ë§ ê°œìˆ˜: {max_notices}ê°œ")

        all_notices = []

        # í˜ì´ì§€ë³„ë¡œ í™•ì¸
        for page in range(1, max_pages + 1):
            # ì´ë¯¸ ìµœëŒ€ ê°œìˆ˜ì— ë„ë‹¬í–ˆìœ¼ë©´ ì¤‘ë‹¨
            if len(all_notices) >= max_notices:
                print(f"[ì •ë³´] ìµœëŒ€ í¬ë¡¤ë§ ê°œìˆ˜({max_notices}ê°œ) ë„ë‹¬ - ì¤‘ë‹¨")
                break

            print(f"\n[í˜ì´ì§€ {page}/{max_pages}] ëª©ë¡ í™•ì¸ ì¤‘...")

            # ëª©ë¡ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
            params = self.BOARD_PARAMS.copy()
            params['startPage'] = str(page)

            soup = self.fetch_page(self.LIST_URL, params=params)

            if not soup:
                print(f"[WARNING] í˜ì´ì§€ {page} ë¡œë“œ ì‹¤íŒ¨")
                break

            # ëª©ë¡ ì¶”ì¶œ
            notices_list = self._extract_notice_list(soup)

            if not notices_list:
                print(f"[INFO] í˜ì´ì§€ {page}ì—ì„œ ê³µì§€ì‚¬í•­ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
                break

            # ìˆœë²ˆ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ ê³µì§€ë§Œ í•„í„°ë§
            new_notices = []
            found_old = False

            for notice in notices_list:
                board_seq = notice.get("board_seq")

                # ìˆœë²ˆì´ ì—†ëŠ” ê²½ìš° (ê³µì§€ ë“±) ë˜ëŠ” ë§ˆì§€ë§‰ ìˆœë²ˆë³´ë‹¤ í° ê²½ìš°ë§Œ ì²˜ë¦¬
                if board_seq is None:
                    # ìˆœë²ˆ ì—†ëŠ” ê³µì§€(ìƒë‹¨ ê³ ì • ë“±)ëŠ” URL ê¸°ë°˜ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬
                    continue

                if last_board_seq and board_seq <= last_board_seq:
                    found_old = True
                    print(f"[ì •ë³´] ê¸°ì¡´ ê³µì§€ ë°œê²¬ (ìˆœë²ˆ {board_seq}) - ì´í›„ ìŠ¤í‚µ")
                    break

                new_notices.append(notice)

            if not new_notices:
                print(f"[INFO] í˜ì´ì§€ {page}ì— ìƒˆ ê³µì§€ ì—†ìŒ")
                break

            # ë‚¨ì€ ìˆ˜ìš© ê°€ëŠ¥ ê°œìˆ˜ë§Œí¼ë§Œ ìë¥´ê¸°
            remaining = max_notices - len(all_notices)
            if len(new_notices) > remaining:
                new_notices = new_notices[:remaining]

            print(f"[OK] ìƒˆ ê³µì§€ {len(new_notices)}ê°œ ë°œê²¬ - ìƒì„¸ í¬ë¡¤ë§ ì‹œì‘")

            # ìƒˆ ê³µì§€ë§Œ ìƒì„¸ í¬ë¡¤ë§
            for i, notice_preview in enumerate(new_notices, 1):
                print(f"  [{i}/{len(new_notices)}] {notice_preview['title'][:30]}...")

                detail = self._crawl_notice_detail(notice_preview)

                if detail:
                    # original_id, source_board, board_seq ì¶”ê°€
                    detail["original_id"] = notice_preview.get("notice_id")
                    detail["source_board"] = self.SOURCE_BOARD
                    detail["board_seq"] = notice_preview.get("board_seq")
                    all_notices.append(detail)

            # ê¸°ì¡´ ê³µì§€ê°€ ë°œê²¬ë˜ë©´ ë” ì´ìƒ í˜ì´ì§€ë¥¼ ëŒì§€ ì•ŠìŒ
            if found_old:
                break

        print(f"\n{'='*50}")
        print(f"[ì™„ë£Œ] ìµœì í™” í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_notices)}ê°œ ìƒˆ ê³µì§€")
        print(f"{'='*50}\n")

        return all_notices

    def _get_last_board_seq(self) -> Optional[int]:
        """
        DBì—ì„œ í•´ë‹¹ ê²Œì‹œíŒì˜ ë§ˆì§€ë§‰ ìˆœë²ˆì„ ì¡°íšŒí•©ë‹ˆë‹¤.

        ğŸ¯ ëª©ì :
        ìˆœë²ˆ ê¸°ë°˜ ì¤‘ë³µ ì²´í¬ë¥¼ ìœ„í•´ DBì— ì €ì¥ëœ ìµœì‹  ìˆœë²ˆì„ ê°€ì ¸ì˜µë‹ˆë‹¤.

        ğŸ“Š ë°˜í™˜ê°’:
        - ë§ˆì§€ë§‰ ìˆœë²ˆ (ì—†ìœ¼ë©´ None)
        """
        try:
            from supabase import create_client
            import os

            url = os.getenv("SUPABASE_URL")
            key = os.getenv("SUPABASE_KEY")

            if not url or not key:
                print("[WARNING] Supabase í™˜ê²½ë³€ìˆ˜ ì—†ìŒ - None ë°˜í™˜")
                return None

            client = create_client(url, key)

            # í•´ë‹¹ ê²Œì‹œíŒì˜ ìµœëŒ€ ìˆœë²ˆ ì¡°íšŒ
            result = client.table("notices")\
                .select("board_seq")\
                .eq("source_board", self.SOURCE_BOARD)\
                .not_.is_("board_seq", "null")\
                .order("board_seq", desc=True)\
                .limit(1)\
                .execute()

            if result.data and result.data[0].get("board_seq"):
                return result.data[0]["board_seq"]

            return None

        except Exception as e:
            print(f"[WARNING] ë§ˆì§€ë§‰ ìˆœë²ˆ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return None

    def crawl(self, max_pages: int = 1, max_notices: int = 10) -> List[Dict[str, Any]]:
        """
        ê³µì§€ì‚¬í•­ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - max_pages: í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: 1)
        - max_notices: ê²Œì‹œíŒë‹¹ ìµœëŒ€ í¬ë¡¤ë§ ê°œìˆ˜ (ê¸°ë³¸ê°’: 10)

        ğŸ¯ í•˜ëŠ” ì¼:
        1. ëª©ë¡ í˜ì´ì§€ì—ì„œ ê³µì§€ì‚¬í•­ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        2. ê° ê³µì§€ì‚¬í•­ì˜ ìƒì„¸ í˜ì´ì§€ ì ‘ì†
        3. ì œëª©, ë‚´ìš©, ì‘ì„±ì¼ ë“± ì¶”ì¶œ
        4. ìµœëŒ€ max_noticesê°œê¹Œì§€ë§Œ ìˆ˜ì§‘í•˜ì—¬ ë°˜í™˜

        ğŸ’¡ ì˜ˆì‹œ:
        crawler = NoticeCrawler()
        notices = crawler.crawl(max_pages=2, max_notices=10)

        for notice in notices:
            print(f"ì œëª©: {notice['title']}")
            print(f"ì‘ì„±ì¼: {notice['published_at']}")
        """
        print(f"\n{'='*50}")
        print(f"[í¬ë¡¤ë§] ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ ì‹œì‘ (ìµœëŒ€ {max_pages}í˜ì´ì§€, ìµœëŒ€ {max_notices}ê°œ)")
        print(f"{'='*50}\n")

        all_notices = []

        # í˜ì´ì§€ë³„ë¡œ í¬ë¡¤ë§
        for page in range(1, max_pages + 1):
            # ì´ë¯¸ ìµœëŒ€ ê°œìˆ˜ì— ë„ë‹¬í–ˆìœ¼ë©´ ì¤‘ë‹¨
            if len(all_notices) >= max_notices:
                print(f"[ì •ë³´] ìµœëŒ€ í¬ë¡¤ë§ ê°œìˆ˜({max_notices}ê°œ) ë„ë‹¬ - ì¤‘ë‹¨")
                break

            print(f"\n[í˜ì´ì§€ {page}/{max_pages}] í¬ë¡¤ë§ ì¤‘...")

            # í˜ì´ì§€ íŒŒë¼ë¯¸í„° ì¶”ê°€
            params = self.BOARD_PARAMS.copy()
            params['startPage'] = str(page)  # í˜ì´ì§€ë„¤ì´ì…˜

            # ëª©ë¡ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
            soup = self.fetch_page(self.LIST_URL, params=params)

            if not soup:
                print(f"[WARNING] í˜ì´ì§€ {page} ë¡œë“œ ì‹¤íŒ¨")
                continue

            # ê³µì§€ì‚¬í•­ ëª©ë¡ ì¶”ì¶œ
            notices = self._extract_notice_list(soup)

            if not notices:
                print(f"[INFO] í˜ì´ì§€ {page}ì—ì„œ ê³µì§€ì‚¬í•­ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
                break

            # ë‚¨ì€ ìˆ˜ìš© ê°€ëŠ¥ ê°œìˆ˜ë§Œí¼ë§Œ ì²˜ë¦¬
            remaining = max_notices - len(all_notices)
            notices_to_crawl = notices[:remaining]

            print(f"[OK] {len(notices)}ê°œ ë°œê²¬, {len(notices_to_crawl)}ê°œ ìƒì„¸ í¬ë¡¤ë§")

            # ê° ê³µì§€ì‚¬í•­ì˜ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            for i, notice_preview in enumerate(notices_to_crawl, 1):
                print(f"  [{i}/{len(notices_to_crawl)}] {notice_preview['title'][:30]}...")

                # ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
                detail = self._crawl_notice_detail(notice_preview)

                if detail:
                    all_notices.append(detail)

            print(f"[OK] í˜ì´ì§€ {page} í¬ë¡¤ë§ ì™„ë£Œ: {len(notices_to_crawl)}ê°œ")

        print(f"\n{'='*50}")
        print(f"[ì™„ë£Œ] ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_notices)}ê°œ ê³µì§€ì‚¬í•­")
        print(f"{'='*50}\n")

        return all_notices

    def _extract_notice_list(self, soup) -> List[Dict[str, Any]]:
        """
        ëª©ë¡ í˜ì´ì§€ì—ì„œ ê³µì§€ì‚¬í•­ ëª©ë¡ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - soup: BeautifulSoup ê°ì²´

        ğŸ¯ í•˜ëŠ” ì¼:
        ê²Œì‹œíŒ ëª©ë¡ì—ì„œ ê° ê³µì§€ì‚¬í•­ì˜ ê¸°ë³¸ ì •ë³´(ì œëª©, ë§í¬, ë‚ ì§œ, ìˆœë²ˆ)ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        ğŸ’¡ ë°˜í™˜ê°’:
        [
            {
                "title": "ì œëª©",
                "url": "ìƒì„¸í˜ì´ì§€ ë§í¬",
                "date": "ì‘ì„±ì¼",
                "notice_id": "ê²Œì‹œë¬¼ ID",
                "board_seq": 5125  # ê²Œì‹œíŒ ë‚´ ìˆœë²ˆ
            },
            ...
        ]
        """
        notices = []

        # ê²Œì‹œíŒ í…Œì´ë¸” ì°¾ê¸°
        # êµ°ì‚°ëŒ€ ê²Œì‹œíŒì€ ë³´í†µ <table> ë˜ëŠ” <div class="board-list"> í˜•íƒœ
        board_rows = soup.select('tbody tr')  # ì¼ë°˜ì ì¸ ê²Œì‹œíŒ êµ¬ì¡°

        if not board_rows:
            # ë‹¤ë¥¸ í˜•íƒœì˜ ê²Œì‹œíŒ êµ¬ì¡° ì‹œë„
            board_rows = soup.select('.board-list li') or soup.select('.notice-list li')

        for row in board_rows:
            try:
                # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ
                title_elem = row.select_one('td.title a') or row.select_one('.title a') or row.select_one('a')

                if not title_elem:
                    continue

                title = self.clean_text(title_elem.get_text())
                link = title_elem.get('href', '')

                # ìƒëŒ€ ê²½ë¡œë©´ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                if link and not link.startswith('http'):
                    link = self.BASE_URL + link

                # ë‚ ì§œ ì¶”ì¶œ
                date_elem = row.select_one('td.date') or row.select_one('.date') or row.select_one('.reg-date')
                date_str = self.clean_text(date_elem.get_text()) if date_elem else None

                # ê²Œì‹œë¬¼ ID ì¶”ì¶œ (URLì—ì„œ)
                notice_id = None
                if 'nttId=' in link:
                    notice_id = link.split('nttId=')[1].split('&')[0]
                elif 'id=' in link:
                    notice_id = link.split('id=')[1].split('&')[0]

                # ìˆœë²ˆ ì¶”ì¶œ (td.pcv_moh_768ì—ì„œ)
                # êµ°ì‚°ëŒ€ ê²Œì‹œíŒ HTML: <td class="pcv_moh_768">5125</td>
                board_seq = None
                seq_elem = row.select_one('td.pcv_moh_768')
                if seq_elem:
                    seq_text = self.clean_text(seq_elem.get_text())
                    # ìˆ«ìë§Œ ì¶”ì¶œ (ê³µì§€ ë“± íŠ¹ìˆ˜ í‘œì‹œ ì œì™¸)
                    if seq_text.isdigit():
                        board_seq = int(seq_text)

                notices.append({
                    "title": title,
                    "url": link,
                    "date": date_str,
                    "notice_id": notice_id,
                    "board_seq": board_seq
                })

            except Exception as e:
                print(f"    [WARNING] ëª©ë¡ í•­ëª© íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                continue

        return notices

    def _crawl_notice_detail(self, notice_preview: Dict[str, Any]) -> Dict[str, Any]:
        """
        ê³µì§€ì‚¬í•­ ìƒì„¸ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - notice_preview: ëª©ë¡ì—ì„œ ê°€ì ¸ì˜¨ ê³µì§€ì‚¬í•­ ê¸°ë³¸ ì •ë³´

        ğŸ¯ í•˜ëŠ” ì¼:
        1. ìƒì„¸ í˜ì´ì§€ ì ‘ì†
        2. ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
        3. ì‘ì„±ì, ì¡°íšŒìˆ˜ ë“± ë©”íƒ€ ì •ë³´ ì¶”ì¶œ
        4. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬

        ğŸ’¡ ë°˜í™˜ê°’:
        {
            "title": "ì œëª©",
            "content": "ë³¸ë¬¸ ë‚´ìš©",
            "published_at": datetime ê°ì²´,
            "source_url": "ì›ë³¸ ë§í¬",
            "category": "ê³µì§€ì‚¬í•­",
            ...
        }
        """
        url = notice_preview.get('url')

        if not url:
            return None

        # ìƒì„¸ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
        soup = self.fetch_page(url)

        if not soup:
            return None

        try:
            # ì œëª© ì¶”ì¶œ - ìƒì„¸ í˜ì´ì§€ì—ì„œ ì™„ì „í•œ ì œëª© ê°€ì ¸ì˜¤ê¸°
            title_elem = (
                soup.select_one('div.bv_title') or
                soup.select_one('.board-view-title') or
                soup.select_one('.view-title') or
                soup.select_one('h3.title') or
                soup.select_one('h2.title')
            )

            if title_elem:
                title = self.clean_text(title_elem.get_text())
            else:
                # ìƒì„¸ í˜ì´ì§€ì—ì„œ ì œëª©ì„ ì°¾ì§€ ëª»í•˜ë©´ ëª©ë¡ ì œëª© ì‚¬ìš©
                title = notice_preview.get('title', '')

            # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ (div.bv_content_textì—ì„œë§Œ ì¶”ì¶œ)
            content_elem = (
                soup.select_one('div.bv_content_text') or
                soup.select_one('.board-view-content') or
                soup.select_one('.view-content') or
                soup.select_one('.cont_box')
            )

            # ë³¸ë¬¸ì„ Markdownìœ¼ë¡œ ë³€í™˜ (í‘œ/êµ¬ì¡° ë³´ì¡´)
            if content_elem:
                # ì´ë¯¸ì§€ src ìƒëŒ€ê²½ë¡œë¥¼ ì ˆëŒ€ê²½ë¡œë¡œ ë³€í™˜ (Markdown ë³€í™˜ ì „)
                for img in content_elem.select('img'):
                    src = img.get('src', '')
                    if src and not src.startswith('http'):
                        img['src'] = self.BASE_URL + src

                content = clean_markdown(md_with_align(
                    str(content_elem),
                    heading_style="ATX",
                    strip=['script', 'style'],
                ))
            else:
                content = ""

            # ë³¸ë¬¸ ë‚´ ì´ë¯¸ì§€ URL ì¶”ì¶œ (ì´ë¯¸ì§€ ê³µì§€ ì²˜ë¦¬ìš©)
            content_images = []
            if content_elem:
                for img in content_elem.select('img'):
                    src = img.get('src', '')
                    if src:
                        content_images.append(src)

            # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ê³  ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ì´ë¯¸ì§€ ê³µì§€ë¡œ í‘œì‹œ
            plain_text_len = len(content_elem.get_text().strip()) if content_elem else 0
            if plain_text_len < 50:
                if content_images:
                    print(f"    [INFO] ì´ë¯¸ì§€ ê³µì§€ ê°ì§€: {len(content_images)}ê°œ ì´ë¯¸ì§€ ë°œê²¬")
                else:
                    print(f"    [WARNING] ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìŒ: {title[:30]}... (ê¸¸ì´: {len(content)})")

            # ì‘ì„±ì¼ ì¶”ì¶œ (ìƒì„¸ í˜ì´ì§€ì˜ div.bv_txt01ì—ì„œ)
            date_str = None
            author = None
            views = None

            bv_txt01 = soup.select_one('div.bv_txt01')
            if bv_txt01:
                import re
                for span in bv_txt01.find_all('span'):
                    span_text = span.get_text()

                    # ì‘ì„±ì¼ ì¶”ì¶œ: "ì‘ì„±ì¼ : 2026-01-22"
                    if 'ì‘ì„±ì¼' in span_text:
                        date_match = re.search(r'(\d{4}-\d{2}-\d{2})', span_text)
                        if date_match:
                            date_str = date_match.group(1)

                    # ì‘ì„±ì ì¶”ì¶œ: "ì‘ì„±ì : ì´ë¬´ê³¼"
                    elif 'ì‘ì„±ì' in span_text:
                        author_match = re.search(r'ì‘ì„±ì\s*:\s*(.+)', span_text)
                        if author_match:
                            author = author_match.group(1).strip()

                    # ì¡°íšŒìˆ˜ ì¶”ì¶œ: "ì¡°íšŒìˆ˜ : 160"
                    elif 'ì¡°íšŒìˆ˜' in span_text:
                        views_match = re.search(r'(\d+)', span_text)
                        if views_match:
                            views = int(views_match.group(1))

            # ìƒì„¸ í˜ì´ì§€ì—ì„œ ëª» ì°¾ìœ¼ë©´ ëª©ë¡ì—ì„œ ê°€ì ¸ì˜¨ ë‚ ì§œ ì‚¬ìš©
            if not date_str:
                date_str = notice_preview.get('date', '')

            # ë‚ ì§œ íŒŒì‹±
            published_at = self.parse_date(date_str)

            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê²½ê³  ë¡œê·¸ ì¶œë ¥ í›„ í˜„ì¬ ì‹œê°„ ì‚¬ìš©
            if not published_at:
                print(f"    [WARNING] ì‘ì„±ì¼ íŒŒì‹± ì‹¤íŒ¨ (í˜„ì¬ ì‹œê°„ ì‚¬ìš©): {title[:30]}...")
                published_at = datetime.now()

            # ì²¨ë¶€íŒŒì¼ URL ì¶”ì¶œ (div.bv_file01ì—ì„œ)
            attachments = []
            bv_file01 = soup.select_one('div.bv_file01')
            if bv_file01:
                # a.down_window ë§í¬ë“¤ ì¶”ì¶œ
                for link in bv_file01.select('a.down_window'):
                    href = link.get('href', '')
                    if href:
                        # ìƒëŒ€ ê²½ë¡œë©´ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                        if not href.startswith('http'):
                            href = self.BASE_URL + href
                        attachments.append(href)

            # ì²¨ë¶€íŒŒì¼ì„ ëª» ì°¾ì•˜ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì‹œë„
            if not attachments:
                attachments = self.extract_attachment_urls(soup)

            # ë°ì´í„° ì €ì¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            notice_data = self.save_to_dict(
                title=title,
                content=content,
                published_at=published_at,
                source_url=url,
                author=author,
                views=views,
                attachments=attachments,
                original_id=notice_preview.get('notice_id')
            )

            # ì´ë¯¸ì§€ ê³µì§€ì¸ ê²½ìš° ì´ë¯¸ì§€ URL ì¶”ê°€
            if content_images:
                notice_data["content_images"] = content_images

            return notice_data

        except Exception as e:
            print(f"    [ERROR] ìƒì„¸ í˜ì´ì§€ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            return None

        finally:
            # BeautifulSoup ê°ì²´ ëª…ì‹œì  í•´ì œ (ë©”ëª¨ë¦¬ ì ˆì•½)
            del soup


# ğŸ§ª í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ§ª êµ°ì‚°ëŒ€í•™êµ ê³µì§€ì‚¬í•­ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    try:
        # 1. í¬ë¡¤ëŸ¬ ìƒì„±
        print("\n[1ë‹¨ê³„] í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”...")
        crawler = NoticeCrawler()

        # 2. ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ (1í˜ì´ì§€ë§Œ)
        print("\n[2ë‹¨ê³„] ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ ì‹œì‘...")
        notices = crawler.crawl(max_pages=1)

        # 3. ê²°ê³¼ ì¶œë ¥
        print("\n[3ë‹¨ê³„] í¬ë¡¤ë§ ê²°ê³¼:")
        print(f"ì´ {len(notices)}ê°œ ê³µì§€ì‚¬í•­ ìˆ˜ì§‘\n")

        for i, notice in enumerate(notices[:3], 1):  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
            print(f"\n{'â”€'*60}")
            print(f"[ê³µì§€ {i}]")
            print(f"ğŸ“Œ ì œëª©: {notice['title']}")
            print(f"ğŸ“… ì‘ì„±ì¼: {notice['published_at']}")
            print(f"ğŸ·ï¸ ì¹´í…Œê³ ë¦¬: {notice['category']}")
            print(f"ğŸ”— ë§í¬: {notice['source_url']}")
            print(f"ğŸ“ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {notice['content'][:100]}...")

            if notice.get('author'):
                print(f"âœï¸ ì‘ì„±ì: {notice['author']}")

            if notice.get('views'):
                print(f"ğŸ‘€ ì¡°íšŒìˆ˜: {notice['views']}")

            if notice.get('attachments'):
                print(f"ğŸ“ ì²¨ë¶€íŒŒì¼: {len(notice['attachments'])}ê°œ")

        print(f"\n{'='*60}")
        print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"{'='*60}")

    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()
