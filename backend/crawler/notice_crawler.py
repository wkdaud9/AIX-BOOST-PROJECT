# -*- coding: utf-8 -*-
"""
êµ°ì‚°ëŒ€í•™êµ ê³µì§€ì‚¬í•­ í¬ë¡¤ëŸ¬

ğŸ¤” ì´ íŒŒì¼ì´ í•˜ëŠ” ì¼:
êµ°ì‚°ëŒ€í•™êµ í™ˆí˜ì´ì§€ì˜ ê³µì§€ì‚¬í•­ ê²Œì‹œíŒì—ì„œ ìµœì‹  ê³µì§€ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.

ğŸ“š ë¹„ìœ :
- í•™êµ ê²Œì‹œíŒ = ê³µì§€ì‚¬í•­ì´ ë¶™ì–´ìˆëŠ” í° ê²Œì‹œíŒ
- ì´ í¬ë¡¤ëŸ¬ = ê²Œì‹œíŒì„ ë³´ê³  ì¤‘ìš”í•œ ê³µì§€ë¥¼ ì‚¬ì§„ ì°ì–´ì„œ ì €ì¥í•˜ëŠ” í•™ìƒ
"""

from .base_crawler import BaseCrawler
from typing import List, Dict, Any
from datetime import datetime


class NoticeCrawler(BaseCrawler):
    """
    êµ°ì‚°ëŒ€í•™êµ ê³µì§€ì‚¬í•­ í¬ë¡¤ëŸ¬

    ğŸ¯ ëª©ì :
    êµ°ì‚°ëŒ€í•™êµ í™ˆí˜ì´ì§€ì˜ ê³µì§€ì‚¬í•­ ê²Œì‹œíŒì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.

    ğŸ—ï¸ ì‘ë™ ë°©ì‹:
    1. ê³µì§€ì‚¬í•­ ëª©ë¡ í˜ì´ì§€ ì ‘ì†
    2. ê° ê³µì§€ì‚¬í•­ì˜ ì œëª©, ì‘ì„±ì¼, ë§í¬ ì¶”ì¶œ
    3. ìƒì„¸ í˜ì´ì§€ ì ‘ì†í•´ì„œ ë³¸ë¬¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
    4. ë°ì´í„° ì •ë¦¬í•´ì„œ ë°˜í™˜
    """

    # êµ°ì‚°ëŒ€í•™êµ ê³µì§€ì‚¬í•­ URL ì„¤ì •
    BASE_URL = "https://www.kunsan.ac.kr"
    LIST_URL = "https://www.kunsan.ac.kr/board/list.kunsan"

    # URL íŒŒë¼ë¯¸í„° (ê²Œì‹œíŒ ì„¤ì •)
    BOARD_PARAMS = {
        "boardId": "BBS_0000008",
        "menuCd": "DOM_000000105001001000",
        "contentsSid": "211",
        "cpath": ""
    }

    def __init__(self):
        """
        ê³µì§€ì‚¬í•­ í¬ë¡¤ëŸ¬ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        ğŸ’¡ ì˜ˆì‹œ:
        crawler = NoticeCrawler()
        ê³µì§€ë“¤ = crawler.crawl(max_pages=3)  # ìµœëŒ€ 3í˜ì´ì§€ í¬ë¡¤ë§
        """
        super().__init__(
            base_url=self.BASE_URL,
            category="ê³µì§€ì‚¬í•­"
        )

    def crawl(self, max_pages: int = 1) -> List[Dict[str, Any]]:
        """
        ê³µì§€ì‚¬í•­ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - max_pages: í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: 1)

        ğŸ¯ í•˜ëŠ” ì¼:
        1. ëª©ë¡ í˜ì´ì§€ì—ì„œ ê³µì§€ì‚¬í•­ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        2. ê° ê³µì§€ì‚¬í•­ì˜ ìƒì„¸ í˜ì´ì§€ ì ‘ì†
        3. ì œëª©, ë‚´ìš©, ì‘ì„±ì¼ ë“± ì¶”ì¶œ
        4. ë¦¬ìŠ¤íŠ¸ë¡œ ì •ë¦¬í•´ì„œ ë°˜í™˜

        ğŸ’¡ ì˜ˆì‹œ:
        crawler = NoticeCrawler()
        notices = crawler.crawl(max_pages=2)

        for notice in notices:
            print(f"ì œëª©: {notice['title']}")
            print(f"ì‘ì„±ì¼: {notice['published_at']}")
        """
        print(f"\n{'='*50}")
        print(f"[í¬ë¡¤ë§] ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ ì‹œì‘ (ìµœëŒ€ {max_pages}í˜ì´ì§€)")
        print(f"{'='*50}\n")

        all_notices = []

        # í˜ì´ì§€ë³„ë¡œ í¬ë¡¤ë§
        for page in range(1, max_pages + 1):
            print(f"\n[í˜ì´ì§€ {page}/{max_pages}] í¬ë¡¤ë§ ì¤‘...")

            # í˜ì´ì§€ íŒŒë¼ë¯¸í„° ì¶”ê°€
            params = self.BOARD_PARAMS.copy()
            params['pagerOffset'] = str((page - 1) * 10)  # í˜ì´ì§€ë„¤ì´ì…˜

            # ëª©ë¡ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
            soup = self.fetch_page(self.LIST_URL, params=params)

            if not soup:
                print(f"[WARNING] í˜ì´ì§€ {page} ë¡œë“œ ì‹¤íŒ¨")
                continue

            # ê³µì§€ì‚¬í•­ ëª©ë¡ ì¶”ì¶œ
            notices = self._extract_notice_list(soup)

            if not notices:
                print(f"[INFO] í˜ì´ì§€ {page}ì—ì„œ ê³µì§€ì‚¬í•­ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
                break

            print(f"[OK] {len(notices)}ê°œ ê³µì§€ì‚¬í•­ ë°œê²¬")

            # ê° ê³µì§€ì‚¬í•­ì˜ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            for i, notice_preview in enumerate(notices, 1):
                print(f"  [{i}/{len(notices)}] {notice_preview['title'][:30]}...")

                # ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
                detail = self._crawl_notice_detail(notice_preview)

                if detail:
                    all_notices.append(detail)

            print(f"[OK] í˜ì´ì§€ {page} í¬ë¡¤ë§ ì™„ë£Œ: {len(notices)}ê°œ")

        print(f"\n{'='*50}")
        print(f"[ì™„ë£Œ] ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_notices)}ê°œ ê³µì§€ì‚¬í•­")
        print(f"{'='*50}\n")

        return all_notices

    def _extract_notice_list(self, soup) -> List[Dict[str, Any]]:
        """
        ëª©ë¡ í˜ì´ì§€ì—ì„œ ê³µì§€ì‚¬í•­ ëª©ë¡ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - soup: BeautifulSoup ê°ì²´

        ğŸ¯ í•˜ëŠ” ì¼:
        ê²Œì‹œíŒ ëª©ë¡ì—ì„œ ê° ê³µì§€ì‚¬í•­ì˜ ê¸°ë³¸ ì •ë³´(ì œëª©, ë§í¬, ë‚ ì§œ)ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        ğŸ’¡ ë°˜í™˜ê°’:
        [
            {
                "title": "ì œëª©",
                "url": "ìƒì„¸í˜ì´ì§€ ë§í¬",
                "date": "ì‘ì„±ì¼",
                "notice_id": "ê²Œì‹œë¬¼ ID"
            },
            ...
        ]
        """
        notices = []

        # ê²Œì‹œíŒ í…Œì´ë¸” ì°¾ê¸°
        # êµ°ì‚°ëŒ€ ê²Œì‹œíŒì€ ë³´í†µ <table> ë˜ëŠ” <div class="board-list"> í˜•íƒœ
        board_rows = soup.select('tbody tr')  # ì¼ë°˜ì ì¸ ê²Œì‹œíŒ êµ¬ì¡°

        if not board_rows:
            # ë‹¤ë¥¸ í˜•íƒœì˜ ê²Œì‹œíŒ êµ¬ì¡° ì‹œë„
            board_rows = soup.select('.board-list li') or soup.select('.notice-list li')

        for row in board_rows:
            try:
                # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ
                title_elem = row.select_one('td.title a') or row.select_one('.title a') or row.select_one('a')

                if not title_elem:
                    continue

                title = self.clean_text(title_elem.get_text())
                link = title_elem.get('href', '')

                # ìƒëŒ€ ê²½ë¡œë©´ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                if link and not link.startswith('http'):
                    link = self.BASE_URL + link

                # ë‚ ì§œ ì¶”ì¶œ
                date_elem = row.select_one('td.date') or row.select_one('.date') or row.select_one('.reg-date')
                date_str = self.clean_text(date_elem.get_text()) if date_elem else None

                # ê²Œì‹œë¬¼ ID ì¶”ì¶œ (URLì—ì„œ)
                notice_id = None
                if 'nttId=' in link:
                    notice_id = link.split('nttId=')[1].split('&')[0]
                elif 'id=' in link:
                    notice_id = link.split('id=')[1].split('&')[0]

                notices.append({
                    "title": title,
                    "url": link,
                    "date": date_str,
                    "notice_id": notice_id
                })

            except Exception as e:
                print(f"    [WARNING] ëª©ë¡ í•­ëª© íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                continue

        return notices

    def _crawl_notice_detail(self, notice_preview: Dict[str, Any]) -> Dict[str, Any]:
        """
        ê³µì§€ì‚¬í•­ ìƒì„¸ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - notice_preview: ëª©ë¡ì—ì„œ ê°€ì ¸ì˜¨ ê³µì§€ì‚¬í•­ ê¸°ë³¸ ì •ë³´

        ğŸ¯ í•˜ëŠ” ì¼:
        1. ìƒì„¸ í˜ì´ì§€ ì ‘ì†
        2. ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
        3. ì‘ì„±ì, ì¡°íšŒìˆ˜ ë“± ë©”íƒ€ ì •ë³´ ì¶”ì¶œ
        4. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬

        ğŸ’¡ ë°˜í™˜ê°’:
        {
            "title": "ì œëª©",
            "content": "ë³¸ë¬¸ ë‚´ìš©",
            "published_at": datetime ê°ì²´,
            "source_url": "ì›ë³¸ ë§í¬",
            "category": "ê³µì§€ì‚¬í•­",
            ...
        }
        """
        url = notice_preview.get('url')

        if not url:
            return None

        # ìƒì„¸ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
        soup = self.fetch_page(url)

        if not soup:
            return None

        try:
            # ì œëª© ì¶”ì¶œ
            title = notice_preview.get('title', '')

            # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
            content_elem = (
                soup.select_one('.board-view-content') or
                soup.select_one('.view-content') or
                soup.select_one('.cont_box') or
                soup.select_one('#content') or
                soup.select_one('.content')
            )

            content = self.clean_text(content_elem.get_text()) if content_elem else ""

            # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ ì‚¬ìš©
            if len(content) < 50:
                content = self.clean_text(soup.get_text())

            # ì‘ì„±ì¼ íŒŒì‹±
            date_str = notice_preview.get('date', '')
            published_at = self.parse_date(date_str)

            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í˜„ì¬ ë‚ ì§œ ì‚¬ìš©
            if not published_at:
                published_at = datetime.now()

            # ì‘ì„±ì ì¶”ì¶œ
            author_elem = soup.select_one('.writer') or soup.select_one('.author')
            author = self.clean_text(author_elem.get_text()) if author_elem else None

            # ì¡°íšŒìˆ˜ ì¶”ì¶œ
            views_elem = soup.select_one('.view-count') or soup.select_one('.hit')
            views = None
            if views_elem:
                views_text = views_elem.get_text()
                # ìˆ«ìë§Œ ì¶”ì¶œ
                import re
                views_match = re.search(r'\d+', views_text)
                if views_match:
                    views = int(views_match.group())

            # ì²¨ë¶€íŒŒì¼ URL ì¶”ì¶œ
            attachments = self.extract_attachment_urls(soup)

            # ë°ì´í„° ì €ì¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            notice_data = self.save_to_dict(
                title=title,
                content=content,
                published_at=published_at,
                source_url=url,
                author=author,
                views=views,
                attachments=attachments,
                notice_id=notice_preview.get('notice_id')
            )

            return notice_data

        except Exception as e:
            print(f"    [ERROR] ìƒì„¸ í˜ì´ì§€ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            return None


# ğŸ§ª í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ§ª êµ°ì‚°ëŒ€í•™êµ ê³µì§€ì‚¬í•­ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    try:
        # 1. í¬ë¡¤ëŸ¬ ìƒì„±
        print("\n[1ë‹¨ê³„] í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”...")
        crawler = NoticeCrawler()

        # 2. ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ (1í˜ì´ì§€ë§Œ)
        print("\n[2ë‹¨ê³„] ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ ì‹œì‘...")
        notices = crawler.crawl(max_pages=1)

        # 3. ê²°ê³¼ ì¶œë ¥
        print("\n[3ë‹¨ê³„] í¬ë¡¤ë§ ê²°ê³¼:")
        print(f"ì´ {len(notices)}ê°œ ê³µì§€ì‚¬í•­ ìˆ˜ì§‘\n")

        for i, notice in enumerate(notices[:3], 1):  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
            print(f"\n{'â”€'*60}")
            print(f"[ê³µì§€ {i}]")
            print(f"ğŸ“Œ ì œëª©: {notice['title']}")
            print(f"ğŸ“… ì‘ì„±ì¼: {notice['published_at']}")
            print(f"ğŸ·ï¸ ì¹´í…Œê³ ë¦¬: {notice['category']}")
            print(f"ğŸ”— ë§í¬: {notice['source_url']}")
            print(f"ğŸ“ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {notice['content'][:100]}...")

            if notice.get('author'):
                print(f"âœï¸ ì‘ì„±ì: {notice['author']}")

            if notice.get('views'):
                print(f"ğŸ‘€ ì¡°íšŒìˆ˜: {notice['views']}")

            if notice.get('attachments'):
                print(f"ğŸ“ ì²¨ë¶€íŒŒì¼: {len(notice['attachments'])}ê°œ")

        print(f"\n{'='*60}")
        print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"{'='*60}")

    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()
