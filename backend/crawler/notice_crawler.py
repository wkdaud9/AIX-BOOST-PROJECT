# -*- coding: utf-8 -*-
"""
êµ°ì‚°ëŒ€í•™êµ ê³µì§€ì‚¬í•­ í¬ë¡¤ëŸ¬

ğŸ¤” ì´ íŒŒì¼ì´ í•˜ëŠ” ì¼:
êµ°ì‚°ëŒ€í•™êµ í™ˆí˜ì´ì§€ì˜ ê³µì§€ì‚¬í•­ ê²Œì‹œíŒì—ì„œ ìµœì‹  ê³µì§€ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.

ğŸ“š ë¹„ìœ :
- í•™êµ ê²Œì‹œíŒ = ê³µì§€ì‚¬í•­ì´ ë¶™ì–´ìˆëŠ” í° ê²Œì‹œíŒ
- ì´ í¬ë¡¤ëŸ¬ = ê²Œì‹œíŒì„ ë³´ê³  ì¤‘ìš”í•œ ê³µì§€ë¥¼ ì‚¬ì§„ ì°ì–´ì„œ ì €ì¥í•˜ëŠ” í•™ìƒ
"""

from .base_crawler import BaseCrawler
from typing import List, Dict, Any, Optional
from datetime import datetime
import sys
import os

# NoticeService importë¥¼ ìœ„í•´ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(__file__)))


class NoticeCrawler(BaseCrawler):
    """
    êµ°ì‚°ëŒ€í•™êµ ê³µì§€ì‚¬í•­ í¬ë¡¤ëŸ¬

    ğŸ¯ ëª©ì :
    êµ°ì‚°ëŒ€í•™êµ í™ˆí˜ì´ì§€ì˜ ê³µì§€ì‚¬í•­ ê²Œì‹œíŒì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.

    ğŸ—ï¸ ì‘ë™ ë°©ì‹:
    1. ê³µì§€ì‚¬í•­ ëª©ë¡ í˜ì´ì§€ ì ‘ì†
    2. ê° ê³µì§€ì‚¬í•­ì˜ ì œëª©, ì‘ì„±ì¼, ë§í¬ ì¶”ì¶œ
    3. ìƒì„¸ í˜ì´ì§€ ì ‘ì†í•´ì„œ ë³¸ë¬¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
    4. ë°ì´í„° ì •ë¦¬í•´ì„œ ë°˜í™˜
    """

    # êµ°ì‚°ëŒ€í•™êµ ê³µì§€ì‚¬í•­ URL ì„¤ì •
    BASE_URL = "https://www.kunsan.ac.kr"
    LIST_URL = "https://www.kunsan.ac.kr/board/list.kunsan"

    # URL íŒŒë¼ë¯¸í„° (ê²Œì‹œíŒ ì„¤ì •)
    BOARD_PARAMS = {
        "boardId": "BBS_0000008",
        "menuCd": "DOM_000000105001001000",
        "contentsSid": "211",
        "cpath": ""
    }

    def __init__(self):
        """
        ê³µì§€ì‚¬í•­ í¬ë¡¤ëŸ¬ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        ğŸ’¡ ì˜ˆì‹œ:
        crawler = NoticeCrawler()
        ê³µì§€ë“¤ = crawler.crawl(max_pages=3)  # ìµœëŒ€ 3í˜ì´ì§€ í¬ë¡¤ë§
        """
        super().__init__(
            base_url=self.BASE_URL,
            category="ê³µì§€ì‚¬í•­"
        )

    def check_new_notices(self, last_known_id: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        ëª©ë¡ í˜ì´ì§€ë§Œ í™•ì¸í•˜ì—¬ ìƒˆë¡œìš´ ê³µì§€ê°€ ìˆëŠ”ì§€ ì²´í¬í•©ë‹ˆë‹¤.

        ğŸ¯ ëª©ì :
        DBì˜ ë§ˆì§€ë§‰ original_idì™€ ë¹„êµí•˜ì—¬ ìƒˆ ê³µì§€ê°€ ìˆì„ ë•Œë§Œ ìƒì„¸ í¬ë¡¤ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        í•™êµ ì„œë²„ ë¶€ë‹´ì„ ìµœì†Œí™”í•˜ê³  í¬ë¡¤ë§ ì†ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - last_known_id: DBì— ì €ì¥ëœ ë§ˆì§€ë§‰ ê³µì§€ ID

        ğŸ“Š ë°˜í™˜ê°’:
        - ìƒˆë¡œìš´ ê³µì§€ì‚¬í•­ ëª©ë¡ (IDì™€ URLë§Œ í¬í•¨)

        ğŸ’¡ ì˜ˆì‹œ:
        crawler = NoticeCrawler()
        new_notices = crawler.check_new_notices(last_known_id="12345")
        if new_notices:
            print(f"{len(new_notices)}ê°œ ìƒˆ ê³µì§€ ë°œê²¬!")
        """
        print(f"\n[ì²´í¬] ìƒˆ ê³µì§€ì‚¬í•­ í™•ì¸ ì¤‘... (ë§ˆì§€ë§‰ ID: {last_known_id})")

        # 1í˜ì´ì§€ ëª©ë¡ë§Œ ê°€ì ¸ì˜¤ê¸°
        params = self.BOARD_PARAMS.copy()
        params['pagerOffset'] = '0'

        soup = self.fetch_page(self.LIST_URL, params=params)

        if not soup:
            print("[ERROR] ëª©ë¡ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨")
            return []

        # ëª©ë¡ ì¶”ì¶œ
        notices = self._extract_notice_list(soup)

        if not notices:
            print("[INFO] ëª©ë¡ì—ì„œ ê³µì§€ì‚¬í•­ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
            return []

        # last_known_idê°€ ì—†ìœ¼ë©´ ëª¨ë“  ê³µì§€ë¥¼ ìƒˆ ê³µì§€ë¡œ ê°„ì£¼
        if not last_known_id:
            print(f"[OK] ì²« í¬ë¡¤ë§ - {len(notices)}ê°œ ëª¨ë‘ ì²˜ë¦¬")
            return notices

        # ìƒˆë¡œìš´ ê³µì§€ë§Œ í•„í„°ë§
        new_notices = []
        for notice in notices:
            notice_id = notice.get("notice_id")

            # ë§ˆì§€ë§‰ ì•Œë ¤ì§„ IDë¥¼ ë§Œë‚˜ë©´ ì¤‘ë‹¨
            if notice_id == last_known_id:
                print(f"[OK] ë§ˆì§€ë§‰ ì €ì¥ ê³µì§€ ë°œê²¬ - {len(new_notices)}ê°œ ìƒˆ ê³µì§€")
                break

            new_notices.append(notice)

        # ëª¨ë“  ê³µì§€ê°€ ìƒˆ ê³µì§€ì¸ ê²½ìš° (ë§ˆì§€ë§‰ IDë¥¼ ì°¾ì§€ ëª»í•¨)
        if len(new_notices) == len(notices):
            print(f"[WARNING] ë§ˆì§€ë§‰ IDë¥¼ ì°¾ì§€ ëª»í•¨ - ëª¨ë“  ê³µì§€ ì²˜ë¦¬ ({len(new_notices)}ê°œ)")

        return new_notices

    def crawl_optimized(
        self,
        last_known_id: Optional[str] = None,
        max_pages: int = 1
    ) -> List[Dict[str, Any]]:
        """
        ìµœì í™”ëœ í¬ë¡¤ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

        ğŸ¯ ëª©ì :
        1. ëª©ë¡ í˜ì´ì§€ë§Œ ë¨¼ì € í™•ì¸
        2. ìƒˆ ê³µì§€ê°€ ìˆì„ ë•Œë§Œ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
        3. í•™êµ ì„œë²„ ë¶€ë‹´ ìµœì†Œí™”

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - last_known_id: DBì— ì €ì¥ëœ ë§ˆì§€ë§‰ ê³µì§€ ID
        - max_pages: ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: 1)

        ğŸ“Š ë°˜í™˜ê°’:
        - í¬ë¡¤ë§í•œ ê³µì§€ì‚¬í•­ ë¦¬ìŠ¤íŠ¸ (ìƒì„¸ ì •ë³´ í¬í•¨)

        ğŸ’¡ ì˜ˆì‹œ:
        from services.notice_service import NoticeService

        service = NoticeService()
        last_id = service.get_latest_original_id(category="ê³µì§€ì‚¬í•­")

        crawler = NoticeCrawler()
        notices = crawler.crawl_optimized(last_known_id=last_id)
        print(f"ìƒˆë¡œìš´ ê³µì§€: {len(notices)}ê°œ")
        """
        print(f"\n{'='*50}")
        print(f"[ìµœì í™” í¬ë¡¤ë§] ì‹œì‘")
        print(f"{'='*50}\n")

        all_notices = []

        # í˜ì´ì§€ë³„ë¡œ í™•ì¸
        for page in range(1, max_pages + 1):
            print(f"\n[í˜ì´ì§€ {page}/{max_pages}] ëª©ë¡ í™•ì¸ ì¤‘...")

            # ëª©ë¡ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
            params = self.BOARD_PARAMS.copy()
            params['pagerOffset'] = str((page - 1) * 10)

            soup = self.fetch_page(self.LIST_URL, params=params)

            if not soup:
                print(f"[WARNING] í˜ì´ì§€ {page} ë¡œë“œ ì‹¤íŒ¨")
                break

            # ëª©ë¡ ì¶”ì¶œ
            notices_list = self._extract_notice_list(soup)

            if not notices_list:
                print(f"[INFO] í˜ì´ì§€ {page}ì—ì„œ ê³µì§€ì‚¬í•­ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
                break

            # ìƒˆë¡œìš´ ê³µì§€ë§Œ í•„í„°ë§
            new_notices = []
            found_last_id = False

            for notice in notices_list:
                notice_id = notice.get("notice_id")

                # ë§ˆì§€ë§‰ ì•Œë ¤ì§„ IDë¥¼ ë§Œë‚˜ë©´ ì¤‘ë‹¨
                if last_known_id and notice_id == last_known_id:
                    print(f"[OK] ë§ˆì§€ë§‰ ì €ì¥ ê³µì§€ ë°œê²¬ - í¬ë¡¤ë§ ì¤‘ë‹¨")
                    found_last_id = True
                    break

                new_notices.append(notice)

            if not new_notices:
                print(f"[INFO] í˜ì´ì§€ {page}ì— ìƒˆ ê³µì§€ ì—†ìŒ")
                if found_last_id:
                    break
                continue

            print(f"[OK] ìƒˆ ê³µì§€ {len(new_notices)}ê°œ ë°œê²¬ - ìƒì„¸ í¬ë¡¤ë§ ì‹œì‘")

            # ìƒˆ ê³µì§€ë§Œ ìƒì„¸ í¬ë¡¤ë§
            for i, notice_preview in enumerate(new_notices, 1):
                print(f"  [{i}/{len(new_notices)}] {notice_preview['title'][:30]}...")

                detail = self._crawl_notice_detail(notice_preview)

                if detail:
                    # original_id ì¶”ê°€
                    detail["original_id"] = notice_preview.get("notice_id")
                    all_notices.append(detail)

            # ë§ˆì§€ë§‰ IDë¥¼ ì°¾ì•˜ìœ¼ë©´ ë” ì´ìƒ í˜ì´ì§€ë¥¼ ëŒì§€ ì•ŠìŒ
            if found_last_id:
                break

        print(f"\n{'='*50}")
        print(f"[ì™„ë£Œ] ìµœì í™” í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_notices)}ê°œ ìƒˆ ê³µì§€")
        print(f"{'='*50}\n")

        return all_notices

    def crawl(self, max_pages: int = 1) -> List[Dict[str, Any]]:
        """
        ê³µì§€ì‚¬í•­ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - max_pages: í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: 1)

        ğŸ¯ í•˜ëŠ” ì¼:
        1. ëª©ë¡ í˜ì´ì§€ì—ì„œ ê³µì§€ì‚¬í•­ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        2. ê° ê³µì§€ì‚¬í•­ì˜ ìƒì„¸ í˜ì´ì§€ ì ‘ì†
        3. ì œëª©, ë‚´ìš©, ì‘ì„±ì¼ ë“± ì¶”ì¶œ
        4. ë¦¬ìŠ¤íŠ¸ë¡œ ì •ë¦¬í•´ì„œ ë°˜í™˜

        ğŸ’¡ ì˜ˆì‹œ:
        crawler = NoticeCrawler()
        notices = crawler.crawl(max_pages=2)

        for notice in notices:
            print(f"ì œëª©: {notice['title']}")
            print(f"ì‘ì„±ì¼: {notice['published_at']}")
        """
        print(f"\n{'='*50}")
        print(f"[í¬ë¡¤ë§] ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ ì‹œì‘ (ìµœëŒ€ {max_pages}í˜ì´ì§€)")
        print(f"{'='*50}\n")

        all_notices = []

        # í˜ì´ì§€ë³„ë¡œ í¬ë¡¤ë§
        for page in range(1, max_pages + 1):
            print(f"\n[í˜ì´ì§€ {page}/{max_pages}] í¬ë¡¤ë§ ì¤‘...")

            # í˜ì´ì§€ íŒŒë¼ë¯¸í„° ì¶”ê°€
            params = self.BOARD_PARAMS.copy()
            params['pagerOffset'] = str((page - 1) * 10)  # í˜ì´ì§€ë„¤ì´ì…˜

            # ëª©ë¡ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
            soup = self.fetch_page(self.LIST_URL, params=params)

            if not soup:
                print(f"[WARNING] í˜ì´ì§€ {page} ë¡œë“œ ì‹¤íŒ¨")
                continue

            # ê³µì§€ì‚¬í•­ ëª©ë¡ ì¶”ì¶œ
            notices = self._extract_notice_list(soup)

            if not notices:
                print(f"[INFO] í˜ì´ì§€ {page}ì—ì„œ ê³µì§€ì‚¬í•­ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
                break

            print(f"[OK] {len(notices)}ê°œ ê³µì§€ì‚¬í•­ ë°œê²¬")

            # ê° ê³µì§€ì‚¬í•­ì˜ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            for i, notice_preview in enumerate(notices, 1):
                print(f"  [{i}/{len(notices)}] {notice_preview['title'][:30]}...")

                # ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
                detail = self._crawl_notice_detail(notice_preview)

                if detail:
                    all_notices.append(detail)

            print(f"[OK] í˜ì´ì§€ {page} í¬ë¡¤ë§ ì™„ë£Œ: {len(notices)}ê°œ")

        print(f"\n{'='*50}")
        print(f"[ì™„ë£Œ] ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_notices)}ê°œ ê³µì§€ì‚¬í•­")
        print(f"{'='*50}\n")

        return all_notices

    def _extract_notice_list(self, soup) -> List[Dict[str, Any]]:
        """
        ëª©ë¡ í˜ì´ì§€ì—ì„œ ê³µì§€ì‚¬í•­ ëª©ë¡ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - soup: BeautifulSoup ê°ì²´

        ğŸ¯ í•˜ëŠ” ì¼:
        ê²Œì‹œíŒ ëª©ë¡ì—ì„œ ê° ê³µì§€ì‚¬í•­ì˜ ê¸°ë³¸ ì •ë³´(ì œëª©, ë§í¬, ë‚ ì§œ)ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        ğŸ’¡ ë°˜í™˜ê°’:
        [
            {
                "title": "ì œëª©",
                "url": "ìƒì„¸í˜ì´ì§€ ë§í¬",
                "date": "ì‘ì„±ì¼",
                "notice_id": "ê²Œì‹œë¬¼ ID"
            },
            ...
        ]
        """
        notices = []

        # ê²Œì‹œíŒ í…Œì´ë¸” ì°¾ê¸°
        # êµ°ì‚°ëŒ€ ê²Œì‹œíŒì€ ë³´í†µ <table> ë˜ëŠ” <div class="board-list"> í˜•íƒœ
        board_rows = soup.select('tbody tr')  # ì¼ë°˜ì ì¸ ê²Œì‹œíŒ êµ¬ì¡°

        if not board_rows:
            # ë‹¤ë¥¸ í˜•íƒœì˜ ê²Œì‹œíŒ êµ¬ì¡° ì‹œë„
            board_rows = soup.select('.board-list li') or soup.select('.notice-list li')

        for row in board_rows:
            try:
                # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ
                title_elem = row.select_one('td.title a') or row.select_one('.title a') or row.select_one('a')

                if not title_elem:
                    continue

                title = self.clean_text(title_elem.get_text())
                link = title_elem.get('href', '')

                # ìƒëŒ€ ê²½ë¡œë©´ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                if link and not link.startswith('http'):
                    link = self.BASE_URL + link

                # ë‚ ì§œ ì¶”ì¶œ
                date_elem = row.select_one('td.date') or row.select_one('.date') or row.select_one('.reg-date')
                date_str = self.clean_text(date_elem.get_text()) if date_elem else None

                # ê²Œì‹œë¬¼ ID ì¶”ì¶œ (URLì—ì„œ)
                notice_id = None
                if 'nttId=' in link:
                    notice_id = link.split('nttId=')[1].split('&')[0]
                elif 'id=' in link:
                    notice_id = link.split('id=')[1].split('&')[0]

                notices.append({
                    "title": title,
                    "url": link,
                    "date": date_str,
                    "notice_id": notice_id
                })

            except Exception as e:
                print(f"    [WARNING] ëª©ë¡ í•­ëª© íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                continue

        return notices

    def _crawl_notice_detail(self, notice_preview: Dict[str, Any]) -> Dict[str, Any]:
        """
        ê³µì§€ì‚¬í•­ ìƒì„¸ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - notice_preview: ëª©ë¡ì—ì„œ ê°€ì ¸ì˜¨ ê³µì§€ì‚¬í•­ ê¸°ë³¸ ì •ë³´

        ğŸ¯ í•˜ëŠ” ì¼:
        1. ìƒì„¸ í˜ì´ì§€ ì ‘ì†
        2. ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
        3. ì‘ì„±ì, ì¡°íšŒìˆ˜ ë“± ë©”íƒ€ ì •ë³´ ì¶”ì¶œ
        4. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬

        ğŸ’¡ ë°˜í™˜ê°’:
        {
            "title": "ì œëª©",
            "content": "ë³¸ë¬¸ ë‚´ìš©",
            "published_at": datetime ê°ì²´,
            "source_url": "ì›ë³¸ ë§í¬",
            "category": "ê³µì§€ì‚¬í•­",
            ...
        }
        """
        url = notice_preview.get('url')

        if not url:
            return None

        # ìƒì„¸ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
        soup = self.fetch_page(url)

        if not soup:
            return None

        try:
            # ì œëª© ì¶”ì¶œ
            title = notice_preview.get('title', '')

            # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ (div.bv_content_textì—ì„œë§Œ ì¶”ì¶œ)
            content_elem = (
                soup.select_one('div.bv_content_text') or
                soup.select_one('.board-view-content') or
                soup.select_one('.view-content') or
                soup.select_one('.cont_box')
            )

            content = self.clean_text(content_elem.get_text()) if content_elem else ""

            # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ê²½ê³ ë§Œ ì¶œë ¥ (ì „ì²´ í…ìŠ¤íŠ¸ ì‚¬ìš© ì•ˆ í•¨)
            if len(content) < 50:
                print(f"    [WARNING] ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìŒ: {title[:30]}... (ê¸¸ì´: {len(content)})")

            # ì‘ì„±ì¼ ì¶”ì¶œ (ìƒì„¸ í˜ì´ì§€ì˜ div.bv_txt01ì—ì„œ)
            date_str = None
            author = None
            views = None

            bv_txt01 = soup.select_one('div.bv_txt01')
            if bv_txt01:
                import re
                for span in bv_txt01.find_all('span'):
                    span_text = span.get_text()

                    # ì‘ì„±ì¼ ì¶”ì¶œ: "ì‘ì„±ì¼ : 2026-01-22"
                    if 'ì‘ì„±ì¼' in span_text:
                        date_match = re.search(r'(\d{4}-\d{2}-\d{2})', span_text)
                        if date_match:
                            date_str = date_match.group(1)

                    # ì‘ì„±ì ì¶”ì¶œ: "ì‘ì„±ì : ì´ë¬´ê³¼"
                    elif 'ì‘ì„±ì' in span_text:
                        author_match = re.search(r'ì‘ì„±ì\s*:\s*(.+)', span_text)
                        if author_match:
                            author = author_match.group(1).strip()

                    # ì¡°íšŒìˆ˜ ì¶”ì¶œ: "ì¡°íšŒìˆ˜ : 160"
                    elif 'ì¡°íšŒìˆ˜' in span_text:
                        views_match = re.search(r'(\d+)', span_text)
                        if views_match:
                            views = int(views_match.group(1))

            # ìƒì„¸ í˜ì´ì§€ì—ì„œ ëª» ì°¾ìœ¼ë©´ ëª©ë¡ì—ì„œ ê°€ì ¸ì˜¨ ë‚ ì§œ ì‚¬ìš©
            if not date_str:
                date_str = notice_preview.get('date', '')

            # ë‚ ì§œ íŒŒì‹±
            published_at = self.parse_date(date_str)

            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê²½ê³  ë¡œê·¸ ì¶œë ¥ í›„ í˜„ì¬ ì‹œê°„ ì‚¬ìš©
            if not published_at:
                print(f"    [WARNING] ì‘ì„±ì¼ íŒŒì‹± ì‹¤íŒ¨ (í˜„ì¬ ì‹œê°„ ì‚¬ìš©): {title[:30]}...")
                published_at = datetime.now()

            # ì²¨ë¶€íŒŒì¼ URL ì¶”ì¶œ (div.bv_file01ì—ì„œ)
            attachments = []
            bv_file01 = soup.select_one('div.bv_file01')
            if bv_file01:
                # a.down_window ë§í¬ë“¤ ì¶”ì¶œ
                for link in bv_file01.select('a.down_window'):
                    href = link.get('href', '')
                    if href:
                        # ìƒëŒ€ ê²½ë¡œë©´ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                        if not href.startswith('http'):
                            href = self.BASE_URL + href
                        attachments.append(href)

            # ì²¨ë¶€íŒŒì¼ì„ ëª» ì°¾ì•˜ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì‹œë„
            if not attachments:
                attachments = self.extract_attachment_urls(soup)

            # ë°ì´í„° ì €ì¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            notice_data = self.save_to_dict(
                title=title,
                content=content,
                published_at=published_at,
                source_url=url,
                author=author,
                views=views,
                attachments=attachments,
                notice_id=notice_preview.get('notice_id')
            )

            return notice_data

        except Exception as e:
            print(f"    [ERROR] ìƒì„¸ í˜ì´ì§€ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            return None


# ğŸ§ª í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ§ª êµ°ì‚°ëŒ€í•™êµ ê³µì§€ì‚¬í•­ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    try:
        # 1. í¬ë¡¤ëŸ¬ ìƒì„±
        print("\n[1ë‹¨ê³„] í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”...")
        crawler = NoticeCrawler()

        # 2. ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ (1í˜ì´ì§€ë§Œ)
        print("\n[2ë‹¨ê³„] ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ ì‹œì‘...")
        notices = crawler.crawl(max_pages=1)

        # 3. ê²°ê³¼ ì¶œë ¥
        print("\n[3ë‹¨ê³„] í¬ë¡¤ë§ ê²°ê³¼:")
        print(f"ì´ {len(notices)}ê°œ ê³µì§€ì‚¬í•­ ìˆ˜ì§‘\n")

        for i, notice in enumerate(notices[:3], 1):  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
            print(f"\n{'â”€'*60}")
            print(f"[ê³µì§€ {i}]")
            print(f"ğŸ“Œ ì œëª©: {notice['title']}")
            print(f"ğŸ“… ì‘ì„±ì¼: {notice['published_at']}")
            print(f"ğŸ·ï¸ ì¹´í…Œê³ ë¦¬: {notice['category']}")
            print(f"ğŸ”— ë§í¬: {notice['source_url']}")
            print(f"ğŸ“ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {notice['content'][:100]}...")

            if notice.get('author'):
                print(f"âœï¸ ì‘ì„±ì: {notice['author']}")

            if notice.get('views'):
                print(f"ğŸ‘€ ì¡°íšŒìˆ˜: {notice['views']}")

            if notice.get('attachments'):
                print(f"ğŸ“ ì²¨ë¶€íŒŒì¼: {len(notice['attachments'])}ê°œ")

        print(f"\n{'='*60}")
        print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"{'='*60}")

    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()
