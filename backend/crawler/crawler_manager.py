# -*- coding: utf-8 -*-
"""
í¬ë¡¤ëŸ¬ í†µí•© ê´€ë¦¬ì

ğŸ¤” ì´ íŒŒì¼ì´ í•˜ëŠ” ì¼:
3ê°œì˜ í¬ë¡¤ëŸ¬(ê³µì§€ì‚¬í•­, í•™ì‚¬/ì¥í•™, ëª¨ì§‘ê³µê³ )ë¥¼ í•œë²ˆì— ê´€ë¦¬í•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤.

ğŸ“š ë¹„ìœ :
- 3ê°œ í¬ë¡¤ëŸ¬ = 3ëª…ì˜ ì¼ê¾¼ (ê°ì ë‹¤ë¥¸ ê²Œì‹œíŒ ë‹´ë‹¹)
- ì´ ë§¤ë‹ˆì € = 3ëª…ì˜ ì¼ê¾¼ì„ ì§€íœ˜í•˜ëŠ” ê´€ë¦¬ì
"""

from typing import List, Dict, Any
from .notice_crawler import NoticeCrawler
from .scholarship_crawler import ScholarshipCrawler
from .recruitment_crawler import RecruitmentCrawler
from datetime import datetime


class CrawlerManager:
    """
    í¬ë¡¤ëŸ¬ í†µí•© ê´€ë¦¬ì

    ğŸ¯ ëª©ì :
    ì—¬ëŸ¬ í¬ë¡¤ëŸ¬ë¥¼ í•œë²ˆì— ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ í†µí•© ê´€ë¦¬í•©ë‹ˆë‹¤.

    ğŸ—ï¸ ì£¼ìš” ê¸°ëŠ¥:
    1. crawl_all: ëª¨ë“  ê²Œì‹œíŒ í¬ë¡¤ë§
    2. crawl_category: íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ í¬ë¡¤ë§
    3. get_statistics: í¬ë¡¤ë§ í†µê³„ ì œê³µ
    """

    def __init__(self):
        """
        í¬ë¡¤ëŸ¬ ë§¤ë‹ˆì €ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        ğŸ’¡ ì˜ˆì‹œ:
        manager = CrawlerManager()
        ëª¨ë“ _ê³µì§€ = manager.crawl_all(max_pages=2)
        """
        # 3ê°œì˜ í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        self.crawlers = {
            "ê³µì§€ì‚¬í•­": NoticeCrawler(),
            "í•™ì‚¬/ì¥í•™": ScholarshipCrawler(),
            "ëª¨ì§‘ê³µê³ ": RecruitmentCrawler()
        }

        print("\n" + "="*60)
        print("âœ… í¬ë¡¤ëŸ¬ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"ğŸ“‹ ê´€ë¦¬ ì¤‘ì¸ í¬ë¡¤ëŸ¬: {', '.join(self.crawlers.keys())}")
        print("="*60 + "\n")

    def crawl_all(self, max_pages: int = 1) -> Dict[str, List[Dict[str, Any]]]:
        """
        ëª¨ë“  ê²Œì‹œíŒì„ í•œë²ˆì— í¬ë¡¤ë§í•©ë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - max_pages: ê° ê²Œì‹œíŒë‹¹ í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜

        ğŸ¯ í•˜ëŠ” ì¼:
        1. ê³µì§€ì‚¬í•­, í•™ì‚¬/ì¥í•™, ëª¨ì§‘ê³µê³  ê²Œì‹œíŒì„ ìˆœì„œëŒ€ë¡œ í¬ë¡¤ë§
        2. ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê²°ê³¼ë¥¼ ë¶„ë¥˜í•´ì„œ ì €ì¥
        3. í†µí•©ëœ ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜

        ğŸ’¡ ì˜ˆì‹œ:
        manager = CrawlerManager()
        ê²°ê³¼ = manager.crawl_all(max_pages=2)

        print(f"ê³µì§€ì‚¬í•­: {len(ê²°ê³¼['ê³µì§€ì‚¬í•­'])}ê°œ")
        print(f"í•™ì‚¬/ì¥í•™: {len(ê²°ê³¼['í•™ì‚¬/ì¥í•™'])}ê°œ")
        print(f"ëª¨ì§‘ê³µê³ : {len(ê²°ê³¼['ëª¨ì§‘ê³µê³ '])}ê°œ")
        """
        print("\n" + "ğŸš€ " + "="*56 + " ğŸš€")
        print("     ì „ì²´ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹œì‘")
        print("ğŸš€ " + "="*56 + " ğŸš€\n")

        all_results = {}
        total_count = 0
        start_time = datetime.now()

        # ê° í¬ë¡¤ëŸ¬ ì‹¤í–‰
        for category, crawler in self.crawlers.items():
            print(f"\n{'â”€'*60}")
            print(f"ğŸ” [{category}] í¬ë¡¤ë§ ì‹œì‘...")
            print(f"{'â”€'*60}")

            try:
                results = crawler.crawl(max_pages=max_pages)
                all_results[category] = results
                total_count += len(results)

                print(f"\nâœ… [{category}] ì™„ë£Œ: {len(results)}ê°œ ìˆ˜ì§‘")

            except Exception as e:
                print(f"\nâŒ [{category}] í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
                all_results[category] = []

        # í†µê³„ ì¶œë ¥
        end_time = datetime.now()
        elapsed = (end_time - start_time).total_seconds()

        print("\n" + "="*60)
        print("ğŸ‰ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ!")
        print("="*60)
        print(f"\nğŸ“Š í¬ë¡¤ë§ í†µê³„:")
        for category, results in all_results.items():
            print(f"  â€¢ {category:15s}: {len(results):4d}ê°œ")
        print(f"\n  ğŸ¯ ì´ í•©ê³„: {total_count}ê°œ")
        print(f"  â±ï¸ ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")
        print("="*60 + "\n")

        return all_results

    def crawl_category(self, category: str, max_pages: int = 1) -> List[Dict[str, Any]]:
        """
        íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ í¬ë¡¤ë§í•©ë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - category: í¬ë¡¤ë§í•  ì¹´í…Œê³ ë¦¬ ("ê³µì§€ì‚¬í•­", "í•™ì‚¬/ì¥í•™", "ëª¨ì§‘ê³µê³ ")
        - max_pages: í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜

        ğŸ¯ í•˜ëŠ” ì¼:
        ì§€ì •í•œ ì¹´í…Œê³ ë¦¬ì˜ í¬ë¡¤ëŸ¬ë§Œ ì‹¤í–‰í•´ì„œ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        ğŸ’¡ ì˜ˆì‹œ:
        manager = CrawlerManager()

        # ê³µì§€ì‚¬í•­ë§Œ í¬ë¡¤ë§
        ê³µì§€ë“¤ = manager.crawl_category("ê³µì§€ì‚¬í•­", max_pages=3)

        # í•™ì‚¬/ì¥í•™ë§Œ í¬ë¡¤ë§
        í•™ì‚¬ê³µì§€ë“¤ = manager.crawl_category("í•™ì‚¬/ì¥í•™", max_pages=2)
        """
        if category not in self.crawlers:
            available = ', '.join(self.crawlers.keys())
            raise ValueError(
                f"âŒ ì˜ëª»ëœ ì¹´í…Œê³ ë¦¬: '{category}'\n"
                f"ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬: {available}"
            )

        print(f"\nğŸ” [{category}] í¬ë¡¤ë§ ì‹œì‘...")

        crawler = self.crawlers[category]
        results = crawler.crawl(max_pages=max_pages)

        print(f"âœ… [{category}] ì™„ë£Œ: {len(results)}ê°œ ìˆ˜ì§‘\n")

        return results

    def get_statistics(self, results: Dict[str, List[Dict[str, Any]]]) -> Dict[str, Any]:
        """
        í¬ë¡¤ë§ ê²°ê³¼ì— ëŒ€í•œ í†µê³„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - results: crawl_all()ì˜ ë°˜í™˜ê°’

        ğŸ¯ í•˜ëŠ” ì¼:
        ì¹´í…Œê³ ë¦¬ë³„, ë‚ ì§œë³„ í†µê³„ë¥¼ ê³„ì‚°í•´ì„œ ë°˜í™˜í•©ë‹ˆë‹¤.

        ğŸ’¡ ì˜ˆì‹œ:
        manager = CrawlerManager()
        ê²°ê³¼ = manager.crawl_all()
        í†µê³„ = manager.get_statistics(ê²°ê³¼)

        print(f"ì´ ê³µì§€ì‚¬í•­: {í†µê³„['total_count']}ê°œ")
        print(f"ì¹´í…Œê³ ë¦¬ë³„: {í†µê³„['by_category']}")
        """
        stats = {
            "total_count": 0,
            "by_category": {},
            "latest_update": None
        }

        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        for category, items in results.items():
            stats["by_category"][category] = len(items)
            stats["total_count"] += len(items)

            # ìµœì‹  ì—…ë°ì´íŠ¸ ë‚ ì§œ ì°¾ê¸°
            for item in items:
                pub_date = item.get("published_at")
                if pub_date:
                    if not stats["latest_update"] or pub_date > stats["latest_update"]:
                        stats["latest_update"] = pub_date

        return stats

    def filter_by_date(
        self,
        results: Dict[str, List[Dict[str, Any]]],
        start_date: datetime = None,
        end_date: datetime = None
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        ë‚ ì§œ ë²”ìœ„ë¡œ ê²°ê³¼ë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - results: í¬ë¡¤ë§ ê²°ê³¼
        - start_date: ì‹œì‘ ë‚ ì§œ (ì´í›„ ê³µì§€ë§Œ)
        - end_date: ì¢…ë£Œ ë‚ ì§œ (ì´ì „ ê³µì§€ë§Œ)

        ğŸ¯ í•˜ëŠ” ì¼:
        ì§€ì •í•œ ë‚ ì§œ ë²”ìœ„ì˜ ê³µì§€ì‚¬í•­ë§Œ í•„í„°ë§í•´ì„œ ë°˜í™˜í•©ë‹ˆë‹¤.

        ï¿½ï¿½ ì˜ˆì‹œ:
        from datetime import datetime, timedelta

        manager = CrawlerManager()
        ê²°ê³¼ = manager.crawl_all()

        # ìµœê·¼ 7ì¼ ê³µì§€ë§Œ í•„í„°ë§
        ì¼ì£¼ì¼ì „ = datetime.now() - timedelta(days=7)
        ìµœì‹ ê³µì§€ = manager.filter_by_date(ê²°ê³¼, start_date=ì¼ì£¼ì¼ì „)
        """
        filtered = {}

        for category, items in results.items():
            filtered_items = []

            for item in items:
                pub_date = item.get("published_at")

                if not pub_date:
                    continue

                # ë‚ ì§œ ë²”ìœ„ í™•ì¸
                if start_date and pub_date < start_date:
                    continue

                if end_date and pub_date > end_date:
                    continue

                filtered_items.append(item)

            filtered[category] = filtered_items

        return filtered

    def search_by_keyword(
        self,
        results: Dict[str, List[Dict[str, Any]]],
        keyword: str
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        í‚¤ì›Œë“œë¡œ ê³µì§€ì‚¬í•­ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - results: í¬ë¡¤ë§ ê²°ê³¼
        - keyword: ê²€ìƒ‰í•  í‚¤ì›Œë“œ

        ğŸ¯ í•˜ëŠ” ì¼:
        ì œëª©ì´ë‚˜ ë‚´ìš©ì— í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê³µì§€ì‚¬í•­ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.

        ğŸ’¡ ì˜ˆì‹œ:
        manager = CrawlerManager()
        ê²°ê³¼ = manager.crawl_all()

        # 'ìˆ˜ê°•ì‹ ì²­' í‚¤ì›Œë“œ ê²€ìƒ‰
        ìˆ˜ê°•ì‹ ì²­_ê³µì§€ = manager.search_by_keyword(ê²°ê³¼, "ìˆ˜ê°•ì‹ ì²­")

        # 'ì¥í•™ê¸ˆ' í‚¤ì›Œë“œ ê²€ìƒ‰
        ì¥í•™ê¸ˆ_ê³µì§€ = manager.search_by_keyword(ê²°ê³¼, "ì¥í•™ê¸ˆ")
        """
        searched = {}

        keyword_lower = keyword.lower()

        for category, items in results.items():
            searched_items = []

            for item in items:
                title = item.get("title", "").lower()
                content = item.get("content", "").lower()

                # ì œëª©ì´ë‚˜ ë‚´ìš©ì— í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                if keyword_lower in title or keyword_lower in content:
                    searched_items.append(item)

            searched[category] = searched_items

        return searched


# ğŸ§ª í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    from datetime import timedelta

    print("=" * 70)
    print("ğŸ§ª í¬ë¡¤ëŸ¬ ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸")
    print("=" * 70)

    try:
        # 1. ë§¤ë‹ˆì € ìƒì„±
        print("\n[1ë‹¨ê³„] í¬ë¡¤ëŸ¬ ë§¤ë‹ˆì € ì´ˆê¸°í™”...")
        manager = CrawlerManager()

        # 2. íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ í¬ë¡¤ë§
        print("\n[2ë‹¨ê³„] ê³µì§€ì‚¬í•­ë§Œ í¬ë¡¤ë§...")
        notices = manager.crawl_category("ê³µì§€ì‚¬í•­", max_pages=1)
        print(f"  ê²°ê³¼: {len(notices)}ê°œ")

        # 3. ì „ì²´ í¬ë¡¤ë§
        print("\n[3ë‹¨ê³„] ì „ì²´ ê²Œì‹œíŒ í¬ë¡¤ë§...")
        all_results = manager.crawl_all(max_pages=1)

        # 4. í†µê³„ í™•ì¸
        print("\n[4ë‹¨ê³„] í†µê³„ í™•ì¸...")
        stats = manager.get_statistics(all_results)
        print(f"\nğŸ“Š í†µê³„:")
        print(f"  ì´ ê³µì§€: {stats['total_count']}ê°œ")
        print(f"  ì¹´í…Œê³ ë¦¬ë³„:")
        for cat, count in stats['by_category'].items():
            print(f"    â€¢ {cat}: {count}ê°œ")

        if stats['latest_update']:
            print(f"  ìµœì‹  ì—…ë°ì´íŠ¸: {stats['latest_update']}")

        # 5. í‚¤ì›Œë“œ ê²€ìƒ‰
        print("\n[5ë‹¨ê³„] í‚¤ì›Œë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸...")
        search_results = manager.search_by_keyword(all_results, "ì•ˆë‚´")
        search_count = sum(len(items) for items in search_results.values())
        print(f"  'ì•ˆë‚´' í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼: {search_count}ê°œ")

        # 6. ë‚ ì§œ í•„í„°ë§
        print("\n[6ë‹¨ê³„] ë‚ ì§œ í•„í„°ë§ í…ŒìŠ¤íŠ¸...")
        week_ago = datetime.now() - timedelta(days=7)
        recent_results = manager.filter_by_date(all_results, start_date=week_ago)
        recent_count = sum(len(items) for items in recent_results.values())
        print(f"  ìµœê·¼ 7ì¼ ê³µì§€: {recent_count}ê°œ")

        print("\n" + "="*70)
        print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("="*70)

    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()
