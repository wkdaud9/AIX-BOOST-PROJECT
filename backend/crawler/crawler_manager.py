# -*- coding: utf-8 -*-
"""
í¬ë¡¤ëŸ¬ í†µí•© ê´€ë¦¬ì

ğŸ¤” ì´ íŒŒì¼ì´ í•˜ëŠ” ì¼:
3ê°œì˜ í¬ë¡¤ëŸ¬(ê³µì§€ì‚¬í•­, í•™ì‚¬/ì¥í•™, ëª¨ì§‘ê³µê³ )ë¥¼ í•œë²ˆì— ê´€ë¦¬í•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤.

ğŸ“š ë¹„ìœ :
- 3ê°œ í¬ë¡¤ëŸ¬ = 3ëª…ì˜ ì¼ê¾¼ (ê°ì ë‹¤ë¥¸ ê²Œì‹œíŒ ë‹´ë‹¹)
- ì´ ë§¤ë‹ˆì € = 3ëª…ì˜ ì¼ê¾¼ì„ ì§€íœ˜í•˜ëŠ” ê´€ë¦¬ì
"""

from typing import List, Dict, Any, Optional
from .notice_crawler import NoticeCrawler
from .scholarship_crawler import ScholarshipCrawler
from .recruitment_crawler import RecruitmentCrawler
from datetime import datetime
import sys
import os

# AI ë¶„ì„ ë° ì„œë¹„ìŠ¤ ëª¨ë“ˆ import
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from ai.analyzer import NoticeAnalyzer
from services.notice_service import NoticeService
from services.calendar_service import CalendarService


class CrawlerManager:
    """
    í¬ë¡¤ëŸ¬ í†µí•© ê´€ë¦¬ì

    ğŸ¯ ëª©ì :
    ì—¬ëŸ¬ í¬ë¡¤ëŸ¬ë¥¼ í•œë²ˆì— ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ í†µí•© ê´€ë¦¬í•©ë‹ˆë‹¤.

    ğŸ—ï¸ ì£¼ìš” ê¸°ëŠ¥:
    1. crawl_all: ëª¨ë“  ê²Œì‹œíŒ í¬ë¡¤ë§
    2. crawl_category: íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ í¬ë¡¤ë§
    3. get_statistics: í¬ë¡¤ë§ í†µê³„ ì œê³µ
    """

    def __init__(self):
        """
        í¬ë¡¤ëŸ¬ ë§¤ë‹ˆì €ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        ğŸ’¡ ì˜ˆì‹œ:
        manager = CrawlerManager()
        ëª¨ë“ _ê³µì§€ = manager.crawl_all(max_pages=2)
        """
        # 3ê°œì˜ í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        self.crawlers = {
            "ê³µì§€ì‚¬í•­": NoticeCrawler(),
            "í•™ì‚¬/ì¥í•™": ScholarshipCrawler(),
            "ëª¨ì§‘ê³µê³ ": RecruitmentCrawler()
        }

        print("\n" + "="*60)
        print("[OK] í¬ë¡¤ëŸ¬ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"[ëª©ë¡] ê´€ë¦¬ ì¤‘ì¸ í¬ë¡¤ëŸ¬: {', '.join(self.crawlers.keys())}")
        print("="*60 + "\n")

    def crawl_all(self, max_pages: int = 1, max_notices: int = 10) -> Dict[str, List[Dict[str, Any]]]:
        """
        ëª¨ë“  ê²Œì‹œíŒì„ í•œë²ˆì— í¬ë¡¤ë§í•©ë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - max_pages: ê° ê²Œì‹œíŒë‹¹ í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜
        - max_notices: ê° ê²Œì‹œíŒë‹¹ ìµœëŒ€ í¬ë¡¤ë§ ê°œìˆ˜ (ê¸°ë³¸ê°’: 10)

        ğŸ¯ í•˜ëŠ” ì¼:
        1. ê³µì§€ì‚¬í•­, í•™ì‚¬/ì¥í•™, ëª¨ì§‘ê³µê³  ê²Œì‹œíŒì„ ìˆœì„œëŒ€ë¡œ í¬ë¡¤ë§
        2. ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ìµœëŒ€ max_noticesê°œê¹Œì§€ ìˆ˜ì§‘
        3. í†µí•©ëœ ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜

        ğŸ’¡ ì˜ˆì‹œ:
        manager = CrawlerManager()
        ê²°ê³¼ = manager.crawl_all(max_pages=2, max_notices=10)

        print(f"ê³µì§€ì‚¬í•­: {len(ê²°ê³¼['ê³µì§€ì‚¬í•­'])}ê°œ")
        print(f"í•™ì‚¬/ì¥í•™: {len(ê²°ê³¼['í•™ì‚¬/ì¥í•™'])}ê°œ")
        print(f"ëª¨ì§‘ê³µê³ : {len(ê²°ê³¼['ëª¨ì§‘ê³µê³ '])}ê°œ")
        """
        print("\n" + "[ì‹œì‘] " + "="*54 + " [ì‹œì‘]")
        print("     ì „ì²´ ê²Œì‹œíŒ í¬ë¡¤ë§ ì‹œì‘")
        print("[ì‹œì‘] " + "="*54 + " [ì‹œì‘]\n")

        all_results = {}
        total_count = 0
        start_time = datetime.now()

        # ê° í¬ë¡¤ëŸ¬ ì‹¤í–‰
        for category, crawler in self.crawlers.items():
            print(f"\n{'â”€'*60}")
            print(f"[ê²€ìƒ‰] [{category}] í¬ë¡¤ë§ ì‹œì‘...")
            print(f"{'â”€'*60}")

            try:
                results = crawler.crawl(max_pages=max_pages, max_notices=max_notices)
                all_results[category] = results
                total_count += len(results)

                print(f"\n[OK] [{category}] ì™„ë£Œ: {len(results)}ê°œ ìˆ˜ì§‘")

            except Exception as e:
                print(f"\n[ERROR] [{category}] í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
                all_results[category] = []

        # í†µê³„ ì¶œë ¥
        end_time = datetime.now()
        elapsed = (end_time - start_time).total_seconds()

        print("\n" + "="*60)
        print("[ì™„ë£Œ] ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ!")
        print("="*60)
        print(f"\n[í†µê³„] í¬ë¡¤ë§ í†µê³„:")
        for category, results in all_results.items():
            print(f"  - {category:15s}: {len(results):4d}ê°œ")
        print(f"\n  [ì´ê³„] ì´ í•©ê³„: {total_count}ê°œ")
        print(f"  [ì‹œê°„] ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")
        print("="*60 + "\n")

        return all_results

    def crawl_category(self, category: str, max_pages: int = 1) -> List[Dict[str, Any]]:
        """
        íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ í¬ë¡¤ë§í•©ë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - category: í¬ë¡¤ë§í•  ì¹´í…Œê³ ë¦¬ ("ê³µì§€ì‚¬í•­", "í•™ì‚¬/ì¥í•™", "ëª¨ì§‘ê³µê³ ")
        - max_pages: í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜

        ğŸ¯ í•˜ëŠ” ì¼:
        ì§€ì •í•œ ì¹´í…Œê³ ë¦¬ì˜ í¬ë¡¤ëŸ¬ë§Œ ì‹¤í–‰í•´ì„œ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

        ğŸ’¡ ì˜ˆì‹œ:
        manager = CrawlerManager()

        # ê³µì§€ì‚¬í•­ë§Œ í¬ë¡¤ë§
        ê³µì§€ë“¤ = manager.crawl_category("ê³µì§€ì‚¬í•­", max_pages=3)

        # í•™ì‚¬/ì¥í•™ë§Œ í¬ë¡¤ë§
        í•™ì‚¬ê³µì§€ë“¤ = manager.crawl_category("í•™ì‚¬/ì¥í•™", max_pages=2)
        """
        if category not in self.crawlers:
            available = ', '.join(self.crawlers.keys())
            raise ValueError(
                f"[ERROR] ì˜ëª»ëœ ì¹´í…Œê³ ë¦¬: '{category}'\n"
                f"ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬: {available}"
            )

        print(f"\n[ê²€ìƒ‰] [{category}] í¬ë¡¤ë§ ì‹œì‘...")

        crawler = self.crawlers[category]
        results = crawler.crawl(max_pages=max_pages)

        print(f"[OK] [{category}] ì™„ë£Œ: {len(results)}ê°œ ìˆ˜ì§‘\n")

        return results

    def get_statistics(self, results: Dict[str, List[Dict[str, Any]]]) -> Dict[str, Any]:
        """
        í¬ë¡¤ë§ ê²°ê³¼ì— ëŒ€í•œ í†µê³„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - results: crawl_all()ì˜ ë°˜í™˜ê°’

        ğŸ¯ í•˜ëŠ” ì¼:
        ì¹´í…Œê³ ë¦¬ë³„, ë‚ ì§œë³„ í†µê³„ë¥¼ ê³„ì‚°í•´ì„œ ë°˜í™˜í•©ë‹ˆë‹¤.

        ğŸ’¡ ì˜ˆì‹œ:
        manager = CrawlerManager()
        ê²°ê³¼ = manager.crawl_all()
        í†µê³„ = manager.get_statistics(ê²°ê³¼)

        print(f"ì´ ê³µì§€ì‚¬í•­: {í†µê³„['total_count']}ê°œ")
        print(f"ì¹´í…Œê³ ë¦¬ë³„: {í†µê³„['by_category']}")
        """
        stats = {
            "total_count": 0,
            "by_category": {},
            "latest_update": None
        }

        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        for category, items in results.items():
            stats["by_category"][category] = len(items)
            stats["total_count"] += len(items)

            # ìµœì‹  ì—…ë°ì´íŠ¸ ë‚ ì§œ ì°¾ê¸°
            for item in items:
                pub_date = item.get("published_at")
                if pub_date:
                    if not stats["latest_update"] or pub_date > stats["latest_update"]:
                        stats["latest_update"] = pub_date

        return stats

    def filter_by_date(
        self,
        results: Dict[str, List[Dict[str, Any]]],
        start_date: datetime = None,
        end_date: datetime = None
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        ë‚ ì§œ ë²”ìœ„ë¡œ ê²°ê³¼ë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - results: í¬ë¡¤ë§ ê²°ê³¼
        - start_date: ì‹œì‘ ë‚ ì§œ (ì´í›„ ê³µì§€ë§Œ)
        - end_date: ì¢…ë£Œ ë‚ ì§œ (ì´ì „ ê³µì§€ë§Œ)

        ğŸ¯ í•˜ëŠ” ì¼:
        ì§€ì •í•œ ë‚ ì§œ ë²”ìœ„ì˜ ê³µì§€ì‚¬í•­ë§Œ í•„í„°ë§í•´ì„œ ë°˜í™˜í•©ë‹ˆë‹¤.

        ï¿½ï¿½ ì˜ˆì‹œ:
        from datetime import datetime, timedelta

        manager = CrawlerManager()
        ê²°ê³¼ = manager.crawl_all()

        # ìµœê·¼ 7ì¼ ê³µì§€ë§Œ í•„í„°ë§
        ì¼ì£¼ì¼ì „ = datetime.now() - timedelta(days=7)
        ìµœì‹ ê³µì§€ = manager.filter_by_date(ê²°ê³¼, start_date=ì¼ì£¼ì¼ì „)
        """
        filtered = {}

        for category, items in results.items():
            filtered_items = []

            for item in items:
                pub_date = item.get("published_at")

                if not pub_date:
                    continue

                # ë‚ ì§œ ë²”ìœ„ í™•ì¸
                if start_date and pub_date < start_date:
                    continue

                if end_date and pub_date > end_date:
                    continue

                filtered_items.append(item)

            filtered[category] = filtered_items

        return filtered

    def search_by_keyword(
        self,
        results: Dict[str, List[Dict[str, Any]]],
        keyword: str
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        í‚¤ì›Œë“œë¡œ ê³µì§€ì‚¬í•­ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - results: í¬ë¡¤ë§ ê²°ê³¼
        - keyword: ê²€ìƒ‰í•  í‚¤ì›Œë“œ

        ğŸ¯ í•˜ëŠ” ì¼:
        ì œëª©ì´ë‚˜ ë‚´ìš©ì— í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê³µì§€ì‚¬í•­ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.

        ğŸ’¡ ì˜ˆì‹œ:
        manager = CrawlerManager()
        ê²°ê³¼ = manager.crawl_all()

        # 'ìˆ˜ê°•ì‹ ì²­' í‚¤ì›Œë“œ ê²€ìƒ‰
        ìˆ˜ê°•ì‹ ì²­_ê³µì§€ = manager.search_by_keyword(ê²°ê³¼, "ìˆ˜ê°•ì‹ ì²­")

        # 'ì¥í•™ê¸ˆ' í‚¤ì›Œë“œ ê²€ìƒ‰
        ì¥í•™ê¸ˆ_ê³µì§€ = manager.search_by_keyword(ê²°ê³¼, "ì¥í•™ê¸ˆ")
        """
        searched = {}

        keyword_lower = keyword.lower()

        for category, items in results.items():
            searched_items = []

            for item in items:
                title = item.get("title", "").lower()
                content = item.get("content", "").lower()

                # ì œëª©ì´ë‚˜ ë‚´ìš©ì— í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
                if keyword_lower in title or keyword_lower in content:
                    searched_items.append(item)

            searched[category] = searched_items

        return searched

    def crawl_and_analyze_all(
        self,
        max_pages: int = 1,
        save_to_db: bool = True,
        create_calendar: bool = True
    ) -> Dict[str, Any]:
        """
        í¬ë¡¤ë§ + AI ë¶„ì„ + DB ì €ì¥ + ìº˜ë¦°ë” ìƒì„±ì„ í•œë²ˆì— ìˆ˜í–‰í•©ë‹ˆë‹¤.

        ğŸ¯ ëª©ì :
        ì™„ì „í•œ ìë™í™” íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        í¬ë¡¤ë§ â†’ AI ë¶„ì„ â†’ DB ì €ì¥ â†’ ìº˜ë¦°ë” ì´ë²¤íŠ¸ ìƒì„±

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - max_pages: ê° ê²Œì‹œíŒë‹¹ í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜
        - save_to_db: DBì— ì €ì¥í• ì§€ ì—¬ë¶€ (ê¸°ë³¸: True)
        - create_calendar: ìº˜ë¦°ë” ì´ë²¤íŠ¸ ìƒì„± ì—¬ë¶€ (ê¸°ë³¸: True)

        ğŸ“Š ë°˜í™˜ê°’:
        {
            "crawled": {...},        # í¬ë¡¤ë§ ê²°ê³¼
            "analyzed": [...],       # AI ë¶„ì„ ê²°ê³¼
            "saved": {...},          # DB ì €ì¥ í†µê³„
            "calendar_events": int,  # ìƒì„±ëœ ìº˜ë¦°ë” ì´ë²¤íŠ¸ ìˆ˜
            "statistics": {...}      # ì „ì²´ í†µê³„
        }

        ğŸ’¡ ì˜ˆì‹œ:
        manager = CrawlerManager()
        result = manager.crawl_and_analyze_all(max_pages=2)
        print(f"ë¶„ì„ ì™„ë£Œ: {len(result['analyzed'])}ê°œ")
        print(f"DB ì €ì¥: {result['saved']['inserted']}ê°œ")
        """
        print("\n" + "="*60)
        print("ğŸš€ í¬ë¡¤ë§ + AI ë¶„ì„ + DB ì €ì¥ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        print("="*60)

        start_time = datetime.now()

        # 1. í¬ë¡¤ë§
        print("\n[1ë‹¨ê³„] í¬ë¡¤ë§ ì¤‘...")
        crawled_results = self.crawl_all(max_pages=max_pages)

        # í¬ë¡¤ë§ ê²°ê³¼ë¥¼ í‰íƒ„í™” (ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ ê³µì§€ë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ)
        all_notices = []
        for category, notices in crawled_results.items():
            for notice in notices:
                notice["category"] = category  # ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¶”ê°€
                all_notices.append(notice)

        print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(all_notices)}ê°œ ê³µì§€ì‚¬í•­")

        if not all_notices:
            print("\nâš ï¸ í¬ë¡¤ë§ëœ ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return {
                "crawled": crawled_results,
                "analyzed": [],
                "saved": {"total": 0, "inserted": 0, "updated": 0, "failed": 0},
                "calendar_events": 0,
                "statistics": {}
            }

        # 2. AI ë¶„ì„
        print(f"\n[2ë‹¨ê³„] AI ë¶„ì„ ì¤‘... ({len(all_notices)}ê°œ)")
        analyzed_notices = []

        try:
            analyzer = NoticeAnalyzer()

            for i, notice in enumerate(all_notices, 1):
                print(f"\n  [{i}/{len(all_notices)}] ë¶„ì„ ì¤‘: {notice.get('title', '')[:40]}...")

                try:
                    # AI ì¢…í•© ë¶„ì„
                    analysis = analyzer.analyze_notice_comprehensive(notice)
                    analyzed_notices.append(analysis)

                except Exception as e:
                    print(f"  âŒ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
                    # ë¶„ì„ ì‹¤íŒ¨í•´ë„ ì›ë³¸ ë°ì´í„°ëŠ” ìœ ì§€
                    notice["analyzed"] = False
                    notice["error"] = str(e)
                    analyzed_notices.append(notice)

            print(f"\nâœ… AI ë¶„ì„ ì™„ë£Œ: {len(analyzed_notices)}ê°œ")

        except Exception as e:
            print(f"\nâŒ AI ë¶„ì„ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            analyzed_notices = all_notices

        # 3. DB ì €ì¥
        saved_stats = {"total": 0, "inserted": 0, "updated": 0, "failed": 0}

        if save_to_db:
            print(f"\n[3ë‹¨ê³„] DB ì €ì¥ ì¤‘... ({len(analyzed_notices)}ê°œ)")

            try:
                notice_service = NoticeService()
                saved_stats = notice_service.batch_save_notices(analyzed_notices)

                print(f"\nâœ… DB ì €ì¥ ì™„ë£Œ:")
                print(f"  - ì‹ ê·œ: {saved_stats['inserted']}ê°œ")
                print(f"  - ì—…ë°ì´íŠ¸: {saved_stats['updated']}ê°œ")
                print(f"  - ì‹¤íŒ¨: {saved_stats['failed']}ê°œ")

            except Exception as e:
                print(f"\nâŒ DB ì €ì¥ ì‹¤íŒ¨: {str(e)}")

        else:
            print("\n[3ë‹¨ê³„] DB ì €ì¥ ê±´ë„ˆëœ€ (save_to_db=False)")

        # 4. ìº˜ë¦°ë” ì´ë²¤íŠ¸ ìƒì„±
        calendar_event_count = 0

        if create_calendar and save_to_db:
            print(f"\n[4ë‹¨ê³„] ìº˜ë¦°ë” ì´ë²¤íŠ¸ ìƒì„± ì¤‘...")

            try:
                calendar_service = CalendarService()

                for notice in analyzed_notices:
                    # ë‚ ì§œ ì •ë³´ê°€ ìˆëŠ” ê³µì§€ì‚¬í•­ë§Œ ì²˜ë¦¬
                    dates = notice.get("dates", {})
                    if not dates or not any(dates.values()):
                        continue

                    # ìº˜ë¦°ë” ì´ë²¤íŠ¸ ìƒì„± (ì‚¬ìš©ì ê´€ì‹¬ ì¹´í…Œê³ ë¦¬ ê¸°ë°˜)
                    try:
                        event_ids = calendar_service.create_calendar_events(
                            notice_id=notice.get("id"),
                            dates=dates,
                            notice_title=notice.get("original_title", ""),
                            category=notice.get("category", "í•™ì‚¬"),
                            user_ids=None  # Noneì´ë©´ ê´€ì‹¬ ì‚¬ìš©ì ìë™ ì¡°íšŒ
                        )
                        calendar_event_count += len(event_ids)

                    except Exception as e:
                        print(f"  âš ï¸ ìº˜ë¦°ë” ì´ë²¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
                        continue

                print(f"\nâœ… ìº˜ë¦°ë” ì´ë²¤íŠ¸ ìƒì„± ì™„ë£Œ: {calendar_event_count}ê°œ")

            except Exception as e:
                print(f"\nâŒ ìº˜ë¦°ë” ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")

        elif not save_to_db:
            print("\n[4ë‹¨ê³„] ìº˜ë¦°ë” ì´ë²¤íŠ¸ ìƒì„± ê±´ë„ˆëœ€ (DB ì €ì¥ í•„ìš”)")
        else:
            print("\n[4ë‹¨ê³„] ìº˜ë¦°ë” ì´ë²¤íŠ¸ ìƒì„± ê±´ë„ˆëœ€ (create_calendar=False)")

        # 5. ìµœì¢… í†µê³„
        end_time = datetime.now()
        elapsed = (end_time - start_time).total_seconds()

        statistics = {
            "total_crawled": len(all_notices),
            "total_analyzed": len([n for n in analyzed_notices if n.get("analyzed")]),
            "total_saved": saved_stats["inserted"] + saved_stats["updated"],
            "total_calendar_events": calendar_event_count,
            "elapsed_time": f"{elapsed:.2f}ì´ˆ",
            "by_category": {
                category: len(notices)
                for category, notices in crawled_results.items()
            }
        }

        print("\n" + "="*60)
        print("âœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        print("="*60)
        print(f"\n[ìµœì¢… í†µê³„]")
        print(f"  - í¬ë¡¤ë§: {statistics['total_crawled']}ê°œ")
        print(f"  - AI ë¶„ì„: {statistics['total_analyzed']}ê°œ")
        print(f"  - DB ì €ì¥: {statistics['total_saved']}ê°œ")
        print(f"  - ìº˜ë¦°ë” ì´ë²¤íŠ¸: {statistics['total_calendar_events']}ê°œ")
        print(f"  - ì†Œìš” ì‹œê°„: {statistics['elapsed_time']}")
        print("="*60 + "\n")

        return {
            "crawled": crawled_results,
            "analyzed": analyzed_notices,
            "saved": saved_stats,
            "calendar_events": calendar_event_count,
            "statistics": statistics
        }

    def analyze_existing_notices(
        self,
        limit: int = 50,
        create_calendar: bool = True
    ) -> Dict[str, Any]:
        """
        DBì— ì´ë¯¸ ì €ì¥ëœ ë¯¸ì²˜ë¦¬ ê³µì§€ì‚¬í•­ì„ AIë¡œ ë¶„ì„í•©ë‹ˆë‹¤.

        ğŸ¯ ëª©ì :
        ì´ì „ì— í¬ë¡¤ë§ë§Œ í•˜ê³  AI ë¶„ì„ì„ í•˜ì§€ ì•Šì€ ê³µì§€ì‚¬í•­ì„ ë¶„ì„í•©ë‹ˆë‹¤.

        ğŸ”§ ë§¤ê°œë³€ìˆ˜:
        - limit: ë¶„ì„í•  ìµœëŒ€ ê°œìˆ˜
        - create_calendar: ìº˜ë¦°ë” ì´ë²¤íŠ¸ ìƒì„± ì—¬ë¶€

        ğŸ“Š ë°˜í™˜ê°’:
        {
            "analyzed": int,      # ë¶„ì„ ì™„ë£Œ ê°œìˆ˜
            "failed": int,        # ë¶„ì„ ì‹¤íŒ¨ ê°œìˆ˜
            "calendar_events": int  # ìƒì„±ëœ ìº˜ë¦°ë” ì´ë²¤íŠ¸ ìˆ˜
        }

        ğŸ’¡ ì˜ˆì‹œ:
        manager = CrawlerManager()
        result = manager.analyze_existing_notices(limit=100)
        print(f"ë¶„ì„ ì™„ë£Œ: {result['analyzed']}ê°œ")
        """
        print("\n" + "="*60)
        print("ğŸ”„ ê¸°ì¡´ ê³µì§€ì‚¬í•­ AI ë¶„ì„ ì‹œì‘")
        print("="*60)

        try:
            # 1. ë¯¸ì²˜ë¦¬ ê³µì§€ì‚¬í•­ ì¡°íšŒ
            notice_service = NoticeService()
            unprocessed = notice_service.get_unprocessed_notices(limit=limit)

            if not unprocessed:
                print("\nâœ… ë¯¸ì²˜ë¦¬ ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
                return {"analyzed": 0, "failed": 0, "calendar_events": 0}

            print(f"\në¯¸ì²˜ë¦¬ ê³µì§€ì‚¬í•­: {len(unprocessed)}ê°œ")

            # 2. AI ë¶„ì„
            analyzer = NoticeAnalyzer()
            calendar_service = CalendarService() if create_calendar else None

            analyzed_count = 0
            failed_count = 0
            calendar_event_count = 0

            for i, notice in enumerate(unprocessed, 1):
                print(f"\n[{i}/{len(unprocessed)}] ë¶„ì„ ì¤‘...")

                try:
                    # AI ë¶„ì„
                    analysis = analyzer.analyze_notice_comprehensive(notice)

                    # DB ì—…ë°ì´íŠ¸
                    success = notice_service.update_ai_analysis(
                        notice_id=notice["id"],
                        analysis_result=analysis
                    )

                    if success:
                        analyzed_count += 1

                        # ìº˜ë¦°ë” ì´ë²¤íŠ¸ ìƒì„±
                        if create_calendar and calendar_service:
                            dates = analysis.get("dates", {})
                            if dates and any(dates.values()):
                                try:
                                    event_ids = calendar_service.create_calendar_events(
                                        notice_id=notice["id"],
                                        dates=dates,
                                        notice_title=notice.get("title", ""),
                                        category=analysis.get("category", "í•™ì‚¬"),
                                        user_ids=None
                                    )
                                    calendar_event_count += len(event_ids)
                                except:
                                    pass

                    else:
                        failed_count += 1

                except Exception as e:
                    print(f"  âŒ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
                    failed_count += 1

            print("\n" + "="*60)
            print("âœ… ê¸°ì¡´ ê³µì§€ì‚¬í•­ ë¶„ì„ ì™„ë£Œ!")
            print("="*60)
            print(f"  - ë¶„ì„ ì™„ë£Œ: {analyzed_count}ê°œ")
            print(f"  - ë¶„ì„ ì‹¤íŒ¨: {failed_count}ê°œ")
            print(f"  - ìº˜ë¦°ë” ì´ë²¤íŠ¸: {calendar_event_count}ê°œ")
            print("="*60 + "\n")

            return {
                "analyzed": analyzed_count,
                "failed": failed_count,
                "calendar_events": calendar_event_count
            }

        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return {"analyzed": 0, "failed": 0, "calendar_events": 0}


# ğŸ§ª í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    from datetime import timedelta

    print("=" * 70)
    print("ğŸ§ª í¬ë¡¤ëŸ¬ ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸")
    print("=" * 70)

    try:
        # 1. ë§¤ë‹ˆì € ìƒì„±
        print("\n[1ë‹¨ê³„] í¬ë¡¤ëŸ¬ ë§¤ë‹ˆì € ì´ˆê¸°í™”...")
        manager = CrawlerManager()

        # 2. íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ í¬ë¡¤ë§
        print("\n[2ë‹¨ê³„] ê³µì§€ì‚¬í•­ë§Œ í¬ë¡¤ë§...")
        notices = manager.crawl_category("ê³µì§€ì‚¬í•­", max_pages=1)
        print(f"  ê²°ê³¼: {len(notices)}ê°œ")

        # 3. ì „ì²´ í¬ë¡¤ë§
        print("\n[3ë‹¨ê³„] ì „ì²´ ê²Œì‹œíŒ í¬ë¡¤ë§...")
        all_results = manager.crawl_all(max_pages=1)

        # 4. í†µê³„ í™•ì¸
        print("\n[4ë‹¨ê³„] í†µê³„ í™•ì¸...")
        stats = manager.get_statistics(all_results)
        print(f"\n[í†µê³„]:")
        print(f"  ì´ ê³µì§€: {stats['total_count']}ê°œ")
        print(f"  ì¹´í…Œê³ ë¦¬ë³„:")
        for cat, count in stats['by_category'].items():
            print(f"    - {cat}: {count}ê°œ")

        if stats['latest_update']:
            print(f"  ìµœì‹  ì—…ë°ì´íŠ¸: {stats['latest_update']}")

        # 5. í‚¤ì›Œë“œ ê²€ìƒ‰
        print("\n[5ë‹¨ê³„] í‚¤ì›Œë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸...")
        search_results = manager.search_by_keyword(all_results, "ì•ˆë‚´")
        search_count = sum(len(items) for items in search_results.values())
        print(f"  'ì•ˆë‚´' í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼: {search_count}ê°œ")

        # 6. ë‚ ì§œ í•„í„°ë§
        print("\n[6ë‹¨ê³„] ë‚ ì§œ í•„í„°ë§ í…ŒìŠ¤íŠ¸...")
        week_ago = datetime.now() - timedelta(days=7)
        recent_results = manager.filter_by_date(all_results, start_date=week_ago)
        recent_count = sum(len(items) for items in recent_results.values())
        print(f"  ìµœê·¼ 7ì¼ ê³µì§€: {recent_count}ê°œ")

        print("\n" + "="*70)
        print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("="*70)

    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()
